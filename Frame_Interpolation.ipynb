{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Frame-Interpolation",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jlee335/cs470_Team7_VideoEnhancer/blob/main/Frame_Interpolation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLoUDwtQY4yW"
      },
      "source": [
        "# 1. Basic Settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Of3IYnH0dxb0",
        "outputId": "c974cf51-0c7d-446d-d78b-ed8e0ef4b56a"
      },
      "source": [
        "#@title Mount google drive\n",
        "# login with your google account and type authorization code to mount on your google drive.\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        " \n",
        "# Specify the directory path where `Frame-interpolation.ipynb` exists.\n",
        "\n",
        "root = '/gdrive/My Drive/Frame_Interp/'\n",
        "root_data = '/gdrive/My Drive/Final_Project/'\n",
        "bench_dir =  '/gdrive/My Drive/Middlebury_Benchmark/videos'\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "530xKDIOesY9"
      },
      "source": [
        "**Import Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-ZNG0VNesGV",
        "outputId": "2806b055-92dc-40e9-f788-3d4d8d31a460"
      },
      "source": [
        "#@title Import libraries\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import cv2 as cv\n",
        " \n",
        "import matplotlib.pyplot as plt\n",
        " \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import cv2\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.datasets import VOCSegmentation, SBDataset\n",
        "from torchvision.datasets.vision import StandardTransform\n",
        "from torchvision.utils import make_grid\n",
        "from torchvision import transforms\n",
        "from torchvision.models.vgg import VGG, vgg16, make_layers\n",
        "from torch.optim import SGD\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from math import log10\n",
        "from torch.autograd import Variable\n",
        "from easydict import EasyDict as edict\n",
        "!pip install pydensecrf\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import datetime\n",
        "import pydensecrf.densecrf as dcrf\n",
        "import pydensecrf.utils as utils\n",
        "\n",
        "from skimage.measure import compare_ssim\n",
        "from skimage.measure import compare_psnr\n",
        " \n",
        "!nvidia-smi\n",
        "!pip install --extra-index-url https://developer.download.nvidia.com/compute/redist nvidia-dali-cuda100\n",
        "\n",
        "from nvidia.dali.pipeline import Pipeline\n",
        "import nvidia.dali.ops as ops\n",
        "import nvidia.dali.types as types\n",
        "\n",
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import glob\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pydensecrf in /usr/local/lib/python3.6/dist-packages (1.0rc3)\n",
            "Thu Dec 10 14:33:27 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.45.01    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0    25W / 300W |      0MiB / 16130MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "Looking in indexes: https://pypi.org/simple, https://developer.download.nvidia.com/compute/redist\n",
            "Requirement already satisfied: nvidia-dali-cuda100 in /usr/local/lib/python3.6/dist-packages (0.28.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "TZgci8gkRSyv"
      },
      "source": [
        "#@title Setting parameters\n",
        " \n",
        "torch.manual_seed(470)\n",
        "torch.cuda.manual_seed(470)\n",
        " \n",
        "args = edict()\n",
        " \n",
        "args.batch_size = 1\n",
        "args.lr = 1e-4\n",
        "args.momentum = 0.9\n",
        "args.weight_decay = 5e-4\n",
        "args.epoch = 10\n",
        "args.tensorboard = True\n",
        "args.gpu = True\n",
        "\n",
        " \n",
        "device = 'cuda' if torch.cuda.is_available() and args.gpu else 'cpu'\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttm3wj9t3Bnt"
      },
      "source": [
        "#@title Create directory name.\r\n",
        "# Create directory name.\r\n",
        "result_dir = Path(root) / 'results'\r\n",
        "result_dir.mkdir(parents=True, exist_ok=True)\r\n",
        "\r\n",
        "data_dir = Path(root_data) / 'data'\r\n",
        "data_dir.mkdir(parents=True, exist_ok=True)\r\n",
        "\r\n",
        "train_dir = data_dir / 'test_data'\r\n",
        "valid_dir = data_dir / 'valid_data'\r\n",
        "\r\n",
        "train_dir.mkdir(parents=True, exist_ok=True)\r\n",
        "valid_dir.mkdir(parents=True, exist_ok=True)\r\n",
        "\r\n",
        "#@title Set directory\r\n",
        "num_trial=0\r\n",
        "result_dir= Path(root) / 'results'\r\n",
        "parent_dir = result_dir / f'trial_{num_trial}'\r\n",
        "\r\n",
        "# Modify parent_dir here if you want to resume from a , or to rename directory.\r\n",
        "# parent_dir = result_dir / 'trial_99'\r\n",
        "\r\n",
        "while parent_dir.is_dir():\r\n",
        "    num_trial = int(parent_dir.name.replace('trial_',''))\r\n",
        "    parent_dir = result_dir / f'trial_{num_trial+1}'\r\n",
        " \r\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "9q9GHGLFnJZ7"
      },
      "source": [
        "#@title Helper functions\n",
        "\n",
        "normalize = transforms.Normalize(mean = [0.485, 0.456, 0.406],std = [0.229, 0.224, 0.225])\n",
        "unnormalize = transforms.Normalize(mean = [-2.118, -2.036, -1.804], std = [4.367, 4.464, 4.444])\n",
        "\n",
        "def optical_flow_warp(image, image_optical_flow):\n",
        "\n",
        "    b, _ , h, w = image.size()\n",
        "\n",
        "    grid = np.meshgrid(range(w), range(h))\n",
        "    grid = np.stack(grid, axis=-1).astype(np.float64)\n",
        "    grid[:, :, 0] = grid[:, :, 0] * 2 / (w - 1) -1\n",
        "    grid[:, :, 1] = grid[:, :, 1] * 2 / (h - 1) -1\n",
        "    grid = grid.transpose(2, 0, 1)\n",
        "    grid = np.tile(grid, (b, 1, 1, 1))\n",
        "    grid = Variable(torch.Tensor(grid))\n",
        "    if image_optical_flow.is_cuda == True:\n",
        "        grid = grid.cuda()\n",
        "\n",
        "    flow_0 = torch.unsqueeze(image_optical_flow[:, 0, :, :] * 31 / (w - 1), dim=1)\n",
        "    flow_1 = torch.unsqueeze(image_optical_flow[:, 1, :, :] * 31 / (h - 1), dim=1)\n",
        "\n",
        "    grid = grid + torch.cat((flow_0, flow_1),1)\n",
        "    grid = grid.transpose(1, 2)\n",
        "    grid = grid.transpose(3, 2)\n",
        "    \n",
        "    output = F.grid_sample(image, grid, padding_mode='border')\n",
        "    return output\n",
        "\n",
        "def space_to_depth(x, block_size):\n",
        "    n, c, h, w = x.size()\n",
        "    unfolded_x = torch.nn.functional.unfold(x, block_size, stride=block_size)\n",
        "    return unfolded_x.view(n, c * block_size ** 2, h // block_size, w // block_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siYvQgXQok3Q"
      },
      "source": [
        "# Tensorboard Settings\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 856
        },
        "id": "GbBl6brJopBK",
        "outputId": "2741fdc0-ed92-4628-9856-be64869aa5a3"
      },
      "source": [
        "\n",
        "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "\n",
        "writer = SummaryWriter(logdir)\n",
        "\n",
        "print()\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Reusing TensorBoard on port 6006 (pid 395), started 0:12:51 ago. (Use '!kill 395' to kill it.)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        (async () => {\n",
              "            const url = await google.colab.kernel.proxyPort(6006, {\"cache\": true});\n",
              "            const iframe = document.createElement('iframe');\n",
              "            iframe.src = url;\n",
              "            iframe.setAttribute('width', '100%');\n",
              "            iframe.setAttribute('height', '800');\n",
              "            iframe.setAttribute('frameborder', 0);\n",
              "            document.body.appendChild(iframe);\n",
              "        })();\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1iZWv-Fk-Bh"
      },
      "source": [
        "# 2. Define Networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "_kOaLexbitsA"
      },
      "source": [
        "#@title FlowNet\n",
        "\n",
        "# https://github.com/amanchadha/FRVSR-GAN/blob/master/FRVSRGAN_Models.py\n",
        "class ConvLeaky(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim):\n",
        "        super(ConvLeaky, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=in_dim, out_channels=out_dim,\n",
        "                               kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=out_dim, out_channels=out_dim,\n",
        "                               kernel_size=3, stride=1, padding=1)\n",
        "        \n",
        "        self.conv3 = nn.Conv2d(in_channels=out_dim, out_channels=out_dim,\n",
        "                               kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        out = self.conv1(input)\n",
        "        out = F.leaky_relu(out, 0.2)\n",
        "        out = self.conv2(out)\n",
        "        out = F.leaky_relu(out, 0.2)\n",
        "        out = self.conv3(out)\n",
        "        out = F.leaky_relu(out, 0.2)\n",
        "        return out\n",
        "\n",
        "\n",
        "class FNetBlock(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, typ):\n",
        "        super(FNetBlock, self).__init__()\n",
        "        self.convleaky = ConvLeaky(in_dim, out_dim)\n",
        "        if typ == \"maxpool\":\n",
        "            self.final = lambda x: F.max_pool2d(x, kernel_size=2, ceil_mode=True)\n",
        "        elif typ == \"bilinear\":\n",
        "            self.final = lambda x: F.interpolate(x, scale_factor=2, mode=\"bilinear\")\n",
        "        else:\n",
        "            raise Exception('Type does not match any of maxpool or bilinear')\n",
        "\n",
        "    def forward(self, input):\n",
        "        out = self.convleaky(input)\n",
        "        out = self.final(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "#@title SR Net\n",
        "def crop(tensor,top,left,h,w):\n",
        "    return tensor[:,:,top:top+h,left:left+w]\n",
        "\n",
        "#https://arxiv.org/pdf/1801.04590.pdf\n",
        "class IFNet(nn.Module):\n",
        "    def __init__(self, in_dim=6):\n",
        "        super(IFNet, self).__init__()\n",
        "        self.e64 =  FNetBlock(in_dim, 64, typ=\"maxpool\")\n",
        "        self.e128 =  FNetBlock(64, 128, typ=\"maxpool\")\n",
        "        self.e256 = FNetBlock(128, 256, typ=\"maxpool\")\n",
        "        self.e512 = FNetBlock(256, 512, typ=\"maxpool\")\n",
        "  \n",
        "        self.d256 = FNetBlock(512, 256, typ=\"bilinear\")\n",
        "        self.d128  = FNetBlock(256, 128, typ=\"bilinear\")\n",
        "        self.d64  = FNetBlock(128, 64, typ=\"bilinear\")\n",
        "        self.d32  = FNetBlock(64, 32, typ=\"bilinear\")\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=32, out_channels=3, kernel_size=3, padding=1)\n",
        "      \n",
        "        self.three = nn.Sequential(\n",
        "          nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1),\n",
        "          nn.ReLU(),\n",
        "          nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1),\n",
        "          nn.ReLU(),\n",
        "          nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1),\n",
        "          nn.ReLU()\n",
        "        )\n",
        "        self.tan = nn.Tanh()\n",
        "        \n",
        "    def forward(self, x):\n",
        "\n",
        "        x = torch.cat(x,dim = 1)\n",
        "        x = self.e64(x)\n",
        "        r1 = x\n",
        "        x = self.e128(x)\n",
        "        r2 = x\n",
        "        x = self.e256(x)\n",
        "        r3 = x\n",
        "        x = self.e512(x)\n",
        "        x = self.d256(x)\n",
        "        x += r3\n",
        "        x = self.d128(x)\n",
        "        x += r2\n",
        "        x = self.d64(x)\n",
        "        x += r1\n",
        "        x = self.d32(x)\n",
        "\n",
        "        x = self.three(x)\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.tan(x)\n",
        "        \n",
        "        return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "gr8bHKjelty4"
      },
      "source": [
        "#@title Discriminator Net \n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Discriminator,self).__init__()\n",
        "    self.l_relu = nn.LeakyReLU()\n",
        "    self.outer_conv = nn.Conv2d(3,64,3)\n",
        "\n",
        "    self.conv_64c = nn.Conv2d(64,64,3,2)\n",
        "    self.bn_64c = nn.BatchNorm2d(64)\n",
        "\n",
        "    self.conv_128e = nn.Conv2d(64,128,3,1)\n",
        "    self.bn_128e = nn.BatchNorm2d(128)\n",
        "\n",
        "    self.conv_128c = nn.Conv2d(128,128,3,2)\n",
        "    self.bn_128c = nn.BatchNorm2d(128)\n",
        "\n",
        "    self.conv_256e = nn.Conv2d(128,256,3,1)\n",
        "    self.bn_256e = nn.BatchNorm2d(256)\n",
        "\n",
        "    self.conv_256c = nn.Conv2d(256,256,3,2)\n",
        "    self.bn_256c = nn.BatchNorm2d(256)\n",
        "\n",
        "    self.conv_512e = nn.Conv2d(256,512,3,1)\n",
        "    self.bn_512e = nn.BatchNorm2d(512)\n",
        "\n",
        "    self.conv_512c = nn.Conv2d(512,512,3,2)\n",
        "    self.bn_512c = nn.BatchNorm2d(512)\n",
        "\n",
        "    self.flatten = nn.AdaptiveAvgPool2d(1)\n",
        "    self.lin1 = nn.Conv2d(512, 1024, kernel_size=1)\n",
        "    self.lin2 = nn.Conv2d(1024, 1, kernel_size=1)\n",
        "    self.sig = nn.Sigmoid()\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.l_relu(self.outer_conv(x))\n",
        "    x = self.l_relu(self.bn_64c(self.conv_64c(x)))\n",
        "    x = self.l_relu(self.bn_128e(self.conv_128e(x)))\n",
        "    x = self.l_relu(self.bn_128c(self.conv_128c(x)))\n",
        "    x = self.l_relu(self.bn_256e(self.conv_256e(x)))\n",
        "    x = self.l_relu(self.bn_256c(self.conv_256c(x)))\n",
        "    x = self.l_relu(self.bn_512e(self.conv_512e(x)))\n",
        "    x = self.l_relu(self.bn_512c(self.conv_512c(x)))\n",
        "\n",
        "    x = self.flatten(x)\n",
        "    x = self.l_relu(self.lin1(x))\n",
        "    x = self.sig(self.lin2(x))\n",
        "    \n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "lKCNvPEG67aA"
      },
      "source": [
        "#@title VGG Cutout\n",
        "class VGG_cutout(nn.Module):\n",
        "    def __init__(self, original_model):\n",
        "        super(VGG_cutout, self).__init__()\n",
        "        self.features = nn.Sequential(nn.Sequential(*list(original_model.children())[0])[:27])\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sl0ZWnn0cTCP"
      },
      "source": [
        "#@title Perceptual loss\n",
        "# https://github.com/dongheehand/SRGAN-PyTorch/blob/master/losses.py\n",
        "class perceptual_loss(nn.Module):\n",
        "    def __init__(self, vgg):\n",
        "        super(perceptual_loss, self).__init__()\n",
        "        self.vgg = vgg\n",
        "        self.criterion = nn.MSELoss()\n",
        "\n",
        "    def forward(self, HR, SR, layer = 'relu5_4'):\n",
        "        hr_feat = self.vgg(HR)\n",
        "        sr_feat = self.vgg(SR)\n",
        "        \n",
        "        return self.criterion(hr_feat, sr_feat), hr_feat, sr_feat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5L-JNv1gMtD"
      },
      "source": [
        "NVIDIA DALI dataloader pipeline configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "G-e3DuBzgM3C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8f8eaf39-13a9-4f30-e18b-971ab4008f55"
      },
      "source": [
        "#@title DALI video pipline config\n",
        "batch_size=3\n",
        "sequence_length=3\n",
        "\n",
        "initial_prefetch_size=16\n",
        "\n",
        "video_directory = os.path.join(train_dir)\n",
        "valid_directory = os.path.join(bench_dir)\n",
        "\n",
        "video_files = [video_directory + '/' + f for f in os.listdir(video_directory)]\n",
        "valid_files = [bench_dir + '/' + f for f in os.listdir(valid_directory)]\n",
        "\n",
        "shuffle=True\n",
        "args.batch_size = batch_size\n",
        "\n",
        "class VideoPipe(Pipeline):\n",
        "    def __init__(self, batch_size, num_threads, device_id, data, shuffle):\n",
        "        super(VideoPipe, self).__init__(batch_size, num_threads, device_id, seed=16)\n",
        "        self.input = ops.VideoReader(device=\"gpu\", filenames=data, sequence_length=sequence_length,\n",
        "                                     shard_id=0, num_shards=1,\n",
        "                                     random_shuffle=shuffle, initial_fill=initial_prefetch_size,pad_last_batch = True)\n",
        "\n",
        "    def define_graph(self):\n",
        "        output = self.input(name=\"Reader\")\n",
        "        return output\n",
        "\n",
        "pipe = VideoPipe(batch_size=batch_size, num_threads=2, device_id=0, data=video_files, shuffle=shuffle)\n",
        "pipe.build()\n",
        "\n",
        "valid_pipe = VideoPipe(batch_size=1, num_threads=2, device_id=0, data=valid_files, shuffle=False)\n",
        "valid_pipe.build()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-1a5e49f035ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Test set pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mpipe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVideoPipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_threads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvideo_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mpipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# Valid Set Pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nvidia/dali/pipeline.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, define_graph)\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefine_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 481\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_names_and_devices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    482\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_built\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Critical error when building pipeline:\nError when constructing operator: VideoReader encountered:\n[/opt/dali/dali/operators/reader/loader/video_loader.cc:285] Assert on \"ret >= 0\" failed: Could not open file /gdrive/My Drive/Final_Project/data/test_data/frame because of Invalid data found when processing input\nStacktrace (100 entries):\n[frame 0]: /usr/local/lib/python3.6/dist-packages/nvidia/dali/libdali_operators.so(+0x4991ee) [0x7ff28e0471ee]\n[frame 1]: /usr/local/lib/python3.6/dist-packages/nvidia/dali/libdali_operators.so(+0x2183d73) [0x7ff28fd31d73]\n[frame 2]: /usr/local/lib/python3.6/dist-packages/nvidia/dali/libdali_operators.so(+0x218c613) [0x7ff28fd3a613]\n[frame 3]: /usr/local/lib/python3.6/dist-packages/nvidia/dali/libdali_operators.so(+0x21be7bc) [0x7ff28fd6c7bc]\n[frame 4]: /usr/local/lib/python3.6/dist-packages/nvidia/dali/libdali_operators.so(+0x21bf8c2) [0x7ff28fd6d8c2]\n[frame 5]: /usr/local/lib/python3.6/dist-packages/nvidia/dali/libdali_operators.so(std::_Function_handler<std::unique_ptr<dali::OperatorBase, std::default_delete<dali::OperatorBase> > (dali::OpSpec const&), std::unique_ptr<dali::OperatorBase, std::default_delete<dali::OperatorBase> > (*)(dali::OpSpec const&)>::_M_invoke(std::_Any_data const&, dali::OpSpec const&)+0xc) [0x7ff28e03f08c]\n[frame 6]: /usr/local/lib/python3.6/dist-packages/nvidia/dali/libdali.so(+0x16f104) [0x7ff28cdff104]\n[frame 7]: /usr/local/lib/python3.6/dist-packages/nvidia/dali/libdali.so(dali::InstantiateOperator(dali::OpSpec const&)+0x264) [0x7ff28cdfea44]\n[frame 8]: /usr/local/lib/python3.6/dist-packages/nvidia/dali/libdali.so(dali::OpGraph::InstantiateOperators()+0xac) [0x7ff28cdbaffc]\n[frame 9]: /usr/local/lib/python3.6/dist-packages/nvidia/dali/libdali.so(dali::Pipeline::Build(std::vector<std::pair<std::string, std::string>, std::allocator<std::pair<std::string, std::string> > >)+0x9e0) [0x7ff28ce1a4d0]\n[frame 10]: /usr/local/lib/python3.6/dist-packages/nvidia/dali/backend_impl.cpython-36m-x86_64-linux-gnu.so(+0x4383f) [0x7ff2afdd083f]\n[frame 11]: /usr/local/lib/python3.6/dist-packages/nvidia/dali/backend_impl.cpython-36m-x86_64-linux-gnu.so(+0x8bda9) [0x7ff2afe18da9]\n[frame 12]: /usr/bin/python3(_PyCFunction_FastCallDict+0x35c) [0x566bbc]\n[frame 13]: /usr/bin/python3() [0x50a433]\n[frame 14]: /usr/bin/python3(_PyEval_EvalFrameDefault+0x444) [0x50beb4]\n[frame 15]: /usr/bin/python3() [0x507be4]\n[frame 16]: /usr/bin/python3() [0x509900]\n[frame 17]: /usr/bin/python3() [0x50a2fd]\n[frame 18]: /usr/bin/python3(_PyEval_EvalFrameDefault+0x444) [0x50beb4]\n[frame 19]: /usr/bin/python3() [0x507be4]\n[frame 20]: /usr/bin/python3() [0x5161c5]\n[frame 21]: /usr/bin/python3() [0x50a12f]\n[frame 22]: /usr/bin/python3(_PyEval_EvalFrameDefault+0x444) [0x50beb4]\n[frame 23]: /usr/bin/python3() [0x507be4]\n[frame 24]: /usr/bin/python3() [0x509900]\n[frame 25]: /usr/bin/python3() [0x50a2fd]\n[frame 26]: /usr/bin/python3(_PyEval_EvalFrameDefault+0x444) [0x50beb4]\n[frame 27]: /usr/bin/python3() [0x507be4]\n[frame 28]: /usr/bin/python3() [0x509900]\n[frame 29]: /usr/bin/python3() [0x50a2fd]\n[frame 30]: /usr/bin/python3(_PyEval_EvalFrameDefault+0x1226) [0x50cc96]\n[frame 31]: /usr/bin/python3() [0x507be4]\n[frame 32]: /usr/bin/python3(_PyFunction_FastCallDict+0x2e2) [0x508ec2]\n[frame 33]: /usr/bin/python3() [0x594a01]\n[frame 34]: /usr/bin/python3(PyObject_Call+0x3e) [0x59fd0e]\n[frame 35]: /usr/bin/python3(_PyEval_EvalFrameDefault+0x17e6) [0x50d256]\n[frame 36]: /usr/bin/python3() [0x507be4]\n[frame 37]: /usr/bin/python3() [0x509900]\n[frame 38]: /usr/bin/python3() [0x50a2fd]\n[frame 39]: /usr/bin/python3(_PyEval_EvalFrameDefault+0x1226) [0x50cc96]\n[frame 40]: /usr/bin/python3() [0x507be4]\n[frame 41]: /usr/bin/python3() [0x509900]\n[frame 42]: /usr/bin/python3() [0x50a2fd]\n[frame 43]: /usr/bin/python3(_PyEval_EvalFrameDefault+0x444) [0x50beb4]\n[frame 44]: /usr/bin/python3() [0x5095c8]\n[frame 45]: /usr/bin/python3() [0x50a2fd]\n[frame 46]: /usr/bin/python3(_PyEval_EvalFrameDefault+0x444) [0x50beb4]\n[frame 47]: /usr/bin/python3() [0x5095c8]\n[frame 48]: /usr/bin/python3() [0x50a2fd]\n[frame 49]: /usr/bin/python3(_PyEval_EvalFrameDefault+0x444) [0x50beb4]\n[frame 50]: /usr/bin/python3() [0x507be4]\n[frame 51]: /usr/bin/python3() [0x588d41]\n[frame 52]: /usr/bin/python3(PyObject_Call+0x3e) [0x59fd0e]\n[frame 53]: /usr/bin/python3(_PyEval_EvalFrameDefault+0x17e6) [0x50d256]\n[frame 54]: /usr/bin/python3() [0x507be4]\n[frame 55]: /usr/bin/python3() [0x588d41]\n[frame 56]: /usr/bin/python3(PyObject_Call+0x3e) [0x59fd0e]\n[frame 57]: /usr/bin/python3(_PyEval_EvalFrameDefault+0x17e6) [0x50d256]\n[frame 58]: /usr/bin/python3() [0x507be4]\n[frame 59]: /usr/bin/python3() [0x509900]\n[frame 60]: /usr/bin/python3() [0x50a2fd]\n[frame 61]: /usr/bin/python3(_PyEval_EvalFrameDefault+0x444) [0x50beb4]\n[frame 62]: /usr/bin/python3() [0x5095c8]\n[frame 63]: /usr/bin/python3() [0x50a2fd]\n[frame 64]: /usr/bin/python3(_PyEval_EvalFrameDefault+0x444) [0x50beb4]\n[frame 65]: /usr/bin/python3() [0x5095c8]\n[frame 66]: /usr/bin/python3() [0x50a2fd]\n[frame 67]: /usr/bin/python3(_PyEval_EvalFrameDefault+0x444) [0x50beb4]\n[frame 68]: /usr/bin/python3() [0x507be4]\n[frame 69]: /usr/bin/python3() [0x588d41]\n[frame 70]: /usr/bin/python3(PyObject_Call+0x3e) [0x59fd0e]\n[frame 71]: /usr/bin/python3(_PyEval_EvalFrameDefault+0x17e6) [0x50d256]\n[frame 72]: /usr/bin/python3() [0x507be4]\n[frame 73]: /usr/bin/python3(_PyFunction_FastCallDict+0x2e2) [0x508ec2]\n[frame 74]: /usr/bin/python3(_PyObject_FastCallDict+0x4f1) [0x5a4c61]\n[frame 75]: /usr/bin/python3() [0x5f03bc]\n[frame 76]: /usr/bin/python3(_PyObject_FastCallKeywords+0x19c) [0x5a9dac]\n[frame 77]: /usr/bin/python3() [0x50a433]\n[frame 78]: /usr/bin/python3(_PyEval_EvalFrameDefault+0x444) [0x50beb4]\n[frame 79]: /usr/bin/python3(_PyFunction_FastCallDict+0xf5) [0x508cd5]\n[frame 80]: /usr/bin/python3() [0x594a01]\n[frame 81]: /usr/bin/python3(PyObject_Call+0x3e) [0x59fd0e]\n[frame 82]: /usr/bin/python3(_PyEval_EvalFrameDefault+0x17e6) [0x50d256]\n[frame 83]: /usr/bin/python3() [0x5095c8]\n[frame 84]: /usr/bin/python3() [0x50a2fd]\n[frame 85]: /usr/bin/python3(_PyEval_EvalFrameDefault+0x444) [0x50beb4]\n[frame 86]: /usr/bin/python3() [0x5095c8]\n[frame 87]: /usr/bin/python3() [0x50a2fd]\n[frame 88]: /usr/bin/python3(_PyEval_EvalFrameDefault+0x444) [0x50beb4]\n[frame 89]: /usr/bin/python3() [0x5095c8]\n[frame 90]: /usr/bin/python3() [0x50a2fd]\n[frame 91]: /usr/bin/python3(_PyEval_EvalFrameDefault+0x444) [0x50beb4]\n[frame 92]: /usr/bin/python3() [0x5095c8]\n[frame 93]: /usr/bin/python3() [0x50a2fd]\n[frame 94]: /usr/bin/python3(_PyEval_EvalFrameDefault+0x444) [0x50beb4]\n[frame 95]: /usr/bin/python3() [0x5095c8]\n[frame 96]: /usr/bin/python3() [0x50a2fd]\n[frame 97]: /usr/bin/python3(_PyEval_EvalFrameDefault+0x444) [0x50beb4]\n[frame 98]: /usr/bin/python3() [0x507be4]\n[frame 99]: /usr/bin/python3() [0x509900]\n\nCurrent pipeline object is no longer valid."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtJIDIE3aMaY"
      },
      "source": [
        "# Training Phase"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZxTmuX2b8kq"
      },
      "source": [
        "GAN training Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "LFyt7wUcSHtn"
      },
      "source": [
        "#@title GAN training code\n",
        "\n",
        "\n",
        "def test_net_valid(generator,epoch_num):\n",
        "    randcrop = torch.nn.Sequential(\n",
        "        torchvision.transforms.RandomCrop((320,400), padding=None, pad_if_needed=False, fill=0, padding_mode='constant'),\n",
        "        )\n",
        "    generator.eval()\n",
        "\n",
        "    test_iter = 10 \n",
        "    \n",
        "    avg_psnr = 0\n",
        "    avg_ssim = 0\n",
        "\n",
        "    for i in range(test_iter):\n",
        "      pipe_out = valid_pipe.run() \n",
        "      pipe_out = pipe_out[0].as_cpu().as_array()\n",
        "      pipe_out = randcrop(torch.from_numpy(pipe_out).permute(0,1,4,2,3))\n",
        "\n",
        "      frame_list = torch.chunk(pipe_out,chunks = pipe_out.shape[1],dim=1) \n",
        "\n",
        "      prev_frame =  frame_list[0].squeeze(1).to(device).float()/255.0\n",
        "      frame      =  frame_list[1].squeeze(1).to(device).float()/255.0\n",
        "      next_frame =  frame_list[2].squeeze(1).to(device).float()/255.0\n",
        "\n",
        "      orig_img = frame.squeeze(0).transpose(0,2).transpose(0,1).cpu().detach().numpy()\n",
        "\n",
        "      prev_frame =  normalize(prev_frame).to(device)\n",
        "      frame      =  normalize(frame).to(device)\n",
        "      next_frame =  normalize(next_frame).to(device)\n",
        "\n",
        "      frame_recon = generator((prev_frame,next_frame)) \n",
        "\n",
        "      n_image = unnormalize(frame_recon)\n",
        "      n_image = n_image.squeeze(0).transpose(0,2).transpose(0,1).cpu().detach().numpy()\n",
        "\n",
        "      psnr = compare_psnr(orig_img,n_image)\n",
        "\n",
        "     # if i == 0:\n",
        "        #ssim = compare_ssim(orig_img,n_image,win_size = 11,multichannel=True)\n",
        "        #avg_ssim = ssim\n",
        "      avg_psnr += psnr/test_iter\n",
        "      \n",
        "\n",
        "    print(f\"Validation set testing: psnr = {avg_psnr:.5f}, ssim = {avg_ssim:.5f}\")\n",
        "    writer.add_scalar(\"PSNR/VALID\",  avg_psnr, epoch_num)\n",
        "    writer.add_scalar(\"SSIM/VALID\",  avg_ssim, epoch_num)\n",
        "\n",
        "from torch.autograd import Variable\n",
        "\n",
        "def train_net_GAN(net,discriminator, resume,n_iter,n_iter_desc,gen_epoc,dis_epoch,load_optim = False,fname = []):\n",
        "    criterion = nn.MSELoss().to(device) \n",
        "\n",
        "    randcrop = torch.nn.Sequential(\n",
        "        torchvision.transforms.RandomCrop(256, padding=None, pad_if_needed=False, fill=0, padding_mode='constant'),\n",
        "        )\n",
        "\n",
        "    epoch = 0\n",
        "    \n",
        "    vgg_net = torchvision.models.vgg19(pretrained=True).to(device)\n",
        "    vgg_net_cut = VGG_cutout(vgg_net)\n",
        "    vgg_net_cut.eval()\n",
        "\n",
        "    VGG_CUT_LOSS = perceptual_loss(vgg_net_cut)\n",
        "\n",
        "    optimizer = torch.optim.Adam(net.parameters(),lr=args.lr)\n",
        "    if load_optim:\n",
        "      optimizer.load_state_dict(torch.load(fname[0]))\n",
        "    net.train()\n",
        " \n",
        "    while epoch < gen_epoch: \n",
        "        if gen_epoch == 0:\n",
        "          continue\n",
        "        t1 = time.time()\n",
        "        first = True\n",
        "        loss_total = 0\n",
        "\n",
        "        for i in range(n_iter):\n",
        "            pipe_out = pipe.run() \n",
        "            pipe_out = pipe_out[0].as_cpu().as_array()\n",
        "            pipe_out = randcrop(torch.from_numpy(pipe_out).permute(0,1,4,2,3))\n",
        "\n",
        "            frame_list = torch.chunk(pipe_out,chunks = pipe_out.shape[1],dim=1) \n",
        " \n",
        "            prev_frame =  frame_list[0].squeeze(1).to(device).float()/255.0\n",
        "            frame =       frame_list[1].squeeze(1).float()/255.0             \n",
        "            next_frame =  frame_list[2].squeeze(1).to(device).float()/255.0\n",
        " \n",
        "            prev_frame =  normalize(prev_frame).to(device)\n",
        "            frame     =   normalize(frame).to(device)\n",
        "            next_frame =  normalize(next_frame).to(device)\n",
        " \n",
        "            frame_recon = net((prev_frame,next_frame))\n",
        "\n",
        "            loss,_,_= VGG_CUT_LOSS(frame,frame_recon)\n",
        " \n",
        "            net.zero_grad()\n",
        " \n",
        "            loss.backward()\n",
        " \n",
        "            optimizer.step()\n",
        "            \n",
        "            if i % 10 == 0:\n",
        "              writer.add_scalar(\"gen-only-loss/train\", loss, epoch*n_iter + i)\n",
        " \n",
        "            loss_total += loss.item()/n_iter\n",
        "\n",
        "        t = time.time() - t1\n",
        "        \n",
        "        print(\"--epoch finished-- \" + str(epoch) + \" time taken: \" + str(t))\n",
        "        print(f'train loss : {loss_total:.5f}')\n",
        "        test_net_valid(net,epoch)\n",
        "\n",
        "        torch.save(net.state_dict(),  result_dir / f'tmp_{type(net).__name__}_.ckpt')\n",
        "        torch.save(optimizer.state_dict(),  result_dir / f'tmp_{type(optimizer).__name__}_.ckpt')\n",
        "        epoch += 1\n",
        "\n",
        "        writer.flush()\n",
        "\n",
        "    vgg_net = torchvision.models.vgg19(pretrained=True).to(device)\n",
        "    vgg_net.eval()\n",
        "    VGG_loss = perceptual_loss(vgg_net)\n",
        "    cross_ent = nn.BCELoss()\n",
        " \n",
        "    real_label = torch.ones((batch_size, 1,1,1)).to(device)\n",
        "    fake_label = torch.zeros((batch_size, 1,1,1)).to(device)\n",
        " \n",
        "    ce_loss = nn.BCELoss()\n",
        " \n",
        "    discriminator.train()\n",
        "\n",
        "    optimizer = torch.optim.Adam(net.parameters(),lr=args.lr * 0.1)\n",
        "    disc_optimizer = torch.optim.Adam(discriminator.parameters(),lr=args.lr * 0.1)\n",
        "    if load_optim:\n",
        "      optimizer.load_state_dict(torch.load(fname[0]))\n",
        "      disc_optimizer.load_state_dict(torch.load(fname[1]))\n",
        "\n",
        "    epoch = 0\n",
        "    torch.save(net.state_dict(),  result_dir / f'disc_{type(net).__name__}_.ckpt')\n",
        "\n",
        "    print(\"TRAINING DISCRIMINATOR NETWORK\")\n",
        "    while epoch < dis_epoch: \n",
        "        t1 = time.time()\n",
        "        first = True\n",
        "        loss_total = 0\n",
        "        L2_Loss = 0\n",
        "        adv_Loss = 0\n",
        "        disc_loss_total = 0\n",
        " \n",
        "        for i in range(n_iter_desc):\n",
        "            pipe_out = pipe.run()\n",
        "            pipe_out = pipe_out[0].as_cpu().as_array()\n",
        "            pipe_out = randcrop(torch.from_numpy(pipe_out).permute(0,1,4,2,3))\n",
        " \n",
        "            frame_list = torch.chunk(pipe_out,chunks = pipe_out.shape[1],dim=1)\n",
        " \n",
        "            prev_frame =  frame_list[0].squeeze(1).to(device).float()/255.0\n",
        "            frame =       frame_list[1].squeeze(1).float()/255.0              \n",
        "            next_frame =  frame_list[2].squeeze(1).to(device).float()/255.0\n",
        " \n",
        "            prev_frame =  normalize(prev_frame).to(device)\n",
        "            frame     =   normalize(frame).to(device)\n",
        "            next_frame =  normalize(next_frame).to(device)\n",
        "\n",
        " \n",
        "            frame_recon = net((Variable(prev_frame),Variable(next_frame)))\n",
        "\n",
        "            discriminator.zero_grad() \n",
        "            \n",
        "            fake_prob = discriminator(frame_recon)\n",
        "            real_prob = discriminator(frame)\n",
        "            \n",
        "\n",
        "            fake_error = ce_loss(fake_prob,fake_label) \n",
        "            real_error = ce_loss(real_prob,real_label) \n",
        " \n",
        "            disc_loss = fake_error + real_error\n",
        "\n",
        "            disc_loss.backward(retain_graph = True)                       \n",
        "\n",
        "            disc_optimizer.step()                    \n",
        "\n",
        "            net.zero_grad()                   \n",
        " \n",
        "            percep_loss, hr_feat, sr_feat = VGG_loss((frame + 1.0) / 2.0, (frame_recon + 1.0) / 2.0, layer = 'relu5_4') \n",
        "            L2loss,_,_ = VGG_CUT_LOSS(frame,frame_recon) \n",
        " \n",
        "            percep_loss *= 0.01\n",
        "            adversarial_loss = cross_ent(discriminator(frame_recon), real_label) * 0.001\n",
        " \n",
        "            total_loss = percep_loss + adversarial_loss + L2loss\n",
        " \n",
        "            total_loss.backward()                         \n",
        "            \n",
        "            optimizer.step()                              \n",
        "\n",
        "            if i % 10 == 0:\n",
        "              writer.add_scalar(\"adv-total_loss/train\",         total_loss, epoch*n_iter_desc + i)\n",
        "              writer.add_scalar(\"adv-adversarial_loss/train\",   adversarial_loss, epoch*n_iter_desc + i)\n",
        "              writer.add_scalar(\"adv-L2loss/train\",             L2loss, epoch*n_iter_desc + i)\n",
        "              writer.add_scalar(\"adv-disc_loss/train\",          disc_loss, epoch*n_iter_desc + i)\n",
        " \n",
        "            loss_total += total_loss.item()/n_iter_desc\n",
        "            adv_Loss += adversarial_loss.item()/n_iter_desc\n",
        "            L2_Loss += L2loss.item()/n_iter_desc\n",
        "            disc_loss_total += disc_loss.item()/n_iter_desc\n",
        "\n",
        "        t = time.time() - t1\n",
        "        \n",
        " \n",
        "        epoch += 1\n",
        "        torch.save(net.state_dict(),  result_dir / f'tmp_{type(net).__name__}_.ckpt')\n",
        "        torch.save(discriminator.state_dict(),  result_dir / f'tmp_{type(discriminator).__name__}_.ckpt')\n",
        "        torch.save(optimizer.state_dict(),  result_dir / f'tmp_{type(optimizer).__name__}_.ckpt')\n",
        "        torch.save(disc_optimizer.state_dict(),  result_dir / f'tmp_dc_{type(disc_optimizer).__name__}_.ckpt')\n",
        "\n",
        "        print(\"--fine grain epoch finished-- \" + str(epoch-1) + \" time taken: \" + str(t))\n",
        "        print(f'loss : {loss_total:.5f} ,adv_loss :{adv_Loss:.5f},l2_loss: {L2_Loss:.5f} disc_loss :{disc_loss_total:.5f}')\n",
        "        test_net_valid(net,epoch-1)\n",
        "        writer.flush()\n",
        "\n",
        "    torch.save(net.state_dict(),  result_dir / f'disc_{type(net).__name__}_.ckpt')\n",
        "    torch.save(discriminator.state_dict(),  result_dir / f'disc_{type(discriminator).__name__}_.ckpt')\n",
        "\n",
        "    torch.save(optimizer.state_dict(),  result_dir / f'disc_{type(optimizer).__name__}_.ckpt')\n",
        "    torch.save(disc_optimizer.state_dict(),  result_dir / f'disc_dc_{type(disc_optimizer).__name__}_.ckpt')\n",
        "    \n",
        "    writer.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "DU6HmIy4Q_Cy"
      },
      "source": [
        "#@title Train Model\n",
        " \n",
        "#Set cont as True if you want to continue the training of gernerator based on existing ckpt\n",
        "#load_disc and load_optimizers are same as cont but for discriminator\n",
        "\n",
        "#Put generator's file name and discriminator's file name, generator optimization file, discriminator optimization file in disc_file and gen_file, G_optim_file, D_optim_file.\n",
        "#ex) disc_file = 'tmp_Discriminator_.ckpt'\n",
        "#ex) gen_file = 'tmp_IFNet_.ckpt'\n",
        "#ex) G_optim_file = result_dir / 'tmp_Adam_.ckpt'\n",
        "#ex) D_optim_file = result_dir / 'tmp_dc_Adam_.ckpt'\n",
        "#Set number of epochs and iterations per epoch\n",
        "\n",
        "#####################################\n",
        "cont = True\n",
        "load_disc = True\n",
        "load_optimizers = True\n",
        "\n",
        "disc_file = ''\n",
        "gen_file = ''\n",
        " \n",
        "G_optim_file = ''\n",
        "D_optim_file = ''\n",
        "\n",
        "fname = [G_optim_file,D_optim_file]\n",
        "\n",
        "\n",
        "gen_epoch = \n",
        "gen_iter = \n",
        " \n",
        "dis_epoch = \n",
        "dis_iter = \n",
        "####################################### \n",
        " \n",
        "if cont:\n",
        "  \n",
        "  model = IFNet().to(device)\n",
        "  desc = Discriminator().to(device)\n",
        " \n",
        " \n",
        "  model.load_state_dict(torch.load(result_dir / gen_file))\n",
        "  if load_disc:\n",
        "    desc.load_state_dict(torch.load(result_dir / disc_file))\n",
        " \n",
        "  train_net_GAN(model,desc,False,gen_iter,dis_iter,gen_epoch,dis_epoch,load_optimizers,fname)\n",
        " \n",
        "else:\n",
        "  model = IFNet().to(device)\n",
        "  desc = Discriminator().to(device)\n",
        " \n",
        "  if load_disc:\n",
        "    desc.load_state_dict(torch.load(result_dir / disc_file))\n",
        " \n",
        "  train_net_GAN(model,desc,False,gen_iter,dis_iter,gen_epoch,dis_epoch,load_optimizers,fname)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUso5OBCa0SS"
      },
      "source": [
        "# Model Test\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOyW2zUh7YjO"
      },
      "source": [
        "#@title Input video\r\n",
        "#Put video file as input in data.\r\n",
        "#ex) data='/gdrive/My Drive/Final_Project/data/test_data/Video1.mp4'\r\n",
        "#Put where to save your frames of your video.\r\n",
        "#We hardly recommend you to MAKE \"new foler\" for frames.\r\n",
        "#DO NOT change part after the path in name.\r\n",
        "#ex)  name= '/gdrive/My Drive/Final_Project/data/test_data/frame/'+str(Number)+'.jpg'\r\n",
        "###########################\r\n",
        "data=''\r\n",
        "###################################\r\n",
        "vc=cv2.VideoCapture(data)\r\n",
        "Number=0\r\n",
        "\r\n",
        "while(True):\r\n",
        "  ret, frame=vc.read()\r\n",
        "  if not (ret):\r\n",
        "    break\r\n",
        "  #####################################  \r\n",
        "  name= ''+str(Number)+'.jpg'\r\n",
        "  #########################################\r\n",
        "  cv2.imwrite(name,frame)\r\n",
        "  Number+=1\r\n",
        "\r\n",
        "vc.release()\r\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhViSRd_fXV3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b5d4ca4-0cfd-45a5-ad31-5125f7130a76"
      },
      "source": [
        "\n",
        "#Set model_file as .ckpt file\n",
        "# ex) model_file = Path(root)/'Models'/'disc_IFNet_1129.ckpt'\n",
        "\n",
        "#model_file ='/gdrive/My Drive/Final_Project/results/disc_IFNet_.ckpt'\n",
        "#You should modify the path mentioned as \"HERE\".\n",
        "#The path in \"HERE\" SHOULD BE SAME!!!!\n",
        "#ex)change \"HERE\" as '/gdrive/My Drive/Final_Project/data/test_data/frame'\n",
        "#DO NOT modify part after \"HERE\"\n",
        "#Modify (size) as the size you want.\n",
        "#(size) SHOULD BE SAME and format of (x,y) and each x and y SHOULD BE DIVIDED TO 8!!!!!\n",
        "#ex) (800,800)\n",
        "\n",
        "##########################\n",
        "model_file =''\n",
        "##################################\n",
        "model = IFNet().to(device)\n",
        "model.load_state_dict(torch.load(result_dir / model_file))\n",
        "model.eval()\n",
        "\n",
        "########################\n",
        "frame_list=os.listdir(\"HERE\")\n",
        "########################\n",
        "\n",
        "\n",
        "for i in range(len(frame_list)-1):\n",
        "  images=[]\n",
        "  ############################################\n",
        "  image1= cv2.imread('HERE'+str(i)+'.jpg')\n",
        "  image1=cv2.resize(image1,(size) )\n",
        "  image2= cv2.imread('HERE'+str(i+1)+'.jpg')\n",
        "  image2=cv2.resize(image2,(size) )\n",
        "  ############################################3\n",
        "  image1_=image1\n",
        "  image2_=image2\n",
        "\n",
        "  loader = transforms.Compose([transforms.ToTensor()])\n",
        "  image1 = loader(image1).float()\n",
        "  image1 = image1.unsqueeze(0).to(device)\n",
        "  images.append(normalize(image1))\n",
        "\n",
        "  loader = transforms.Compose([transforms.ToTensor()])\n",
        "  image2 = loader(image2).float()\n",
        "  image2 = image2.unsqueeze(0).to(device)\n",
        "  images.append(normalize(image2))\n",
        "\n",
        "  n_image = model(images)\n",
        "  n_image = unnormalize(n_image)\n",
        " \n",
        "  n_image2 = n_image.squeeze(0).transpose(0,2).transpose(0,1).cpu().detach().numpy()\n",
        "  n_image2 =(n_image2*255).astype(np.uint8)\n",
        "\n",
        "####################################\n",
        "  svname='HERE'+str(i)+'-1.jpg'\n",
        "  cv.imwrite(svname,n_image2)\n",
        "  svname1='HERE'+str(i)+'.jpg'\n",
        "  cv.imwrite(svname1,image1_)\n",
        "  svname2='HERE'+str(i+1)+'.jpg'\n",
        "####################################\n",
        "  cv.imwrite(svname2,image2_)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3063: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSA15UBoT3cf"
      },
      "source": [
        "#in VideoWriter, you should put code where to save your result video file.\r\n",
        "#ex) video=cv2.VideoWriter('/gdrive/My Drive/VideoCapture/capture/Hi.avi',fourcc,15, (800,600))\r\n",
        "\r\n",
        "#fourccis avi file's codec. Divx, Xvid, H.264 is available if you set your video file as .avi.\r\n",
        "#If you want your output as .mp4 change fourcc 'XVID' as'mp4v' and change your video file name as .mp4.\r\n",
        "#You can change FPS of video as you want.\r\n",
        "\r\n",
        "#########################\r\n",
        "fourcc = cv2.VideoWriter_fourcc(*'XVID')\r\n",
        "FPS=\r\n",
        "video=cv2.VideoWriter('/gdrive/My Drive/Final_Project/data/test_data/result/Result.avi',fourcc,FPS, (800,800))\r\n",
        "\r\n",
        "n=os.listdir('HERE')\r\n",
        "\r\n",
        "###################################\r\n",
        "\r\n",
        "def number_from_filename (fname):\r\n",
        "  num = fname.split('.')[0]\r\n",
        "  num2 = fname.split('-')\r\n",
        "  if len(num2) != 1:\r\n",
        "    num = float(num2[0]) + 0.5\r\n",
        "  else:\r\n",
        "    num = float(num)\r\n",
        "  return num\r\n",
        "\r\n",
        "n = sorted(n, key = lambda x : number_from_filename(x))\r\n",
        "\r\n",
        "\r\n",
        "print(n)\r\n",
        "for i in range (len(n)):\r\n",
        "  ####################################3\r\n",
        "  img=cv2.imread('HERE'+n[i])\r\n",
        "  ###############################\r\n",
        "  video.write(img)\r\n",
        "\r\n",
        "video.release()\r\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}