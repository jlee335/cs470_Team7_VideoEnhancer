{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Attempt 2",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jlee335/cs470_Team7_VideoEnhancer/blob/main/Test/Attempt_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tBigEXQQrMs"
      },
      "source": [
        "##<Attempt 2>\r\n",
        "#####This code is composed with\r\n",
        "\r\n",
        "1.   Basic settings\r\n",
        "2.   Tensorboard Settings\r\n",
        "3.   Define Network\r\n",
        "4.   Training Phase\r\n",
        "5.   Model test\r\n",
        "\r\n",
        "#####You should fill the part surrounded by ##################.\r\n",
        "##### Mostly it is path and model file, training settings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLoUDwtQY4yW"
      },
      "source": [
        "# 1. Basic Settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Of3IYnH0dxb0",
        "outputId": "020f13e7-9326-4c43-807b-9810860156a9"
      },
      "source": [
        "#@title Mount google drive\n",
        "# login with your google account and type authorization code to mount on your google drive.\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        " \n",
        "# Specify the directory path where `Attempt2.ipynb` exists.\n",
        "# ex) root = '/gdrive/My Drive/Final_Project/'\n",
        "###########################\n",
        "root = ''\n",
        "###########################"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "530xKDIOesY9"
      },
      "source": [
        "**Import Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-ZNG0VNesGV",
        "outputId": "9168e8a3-b797-416d-c2f8-9cd4a4e891c7"
      },
      "source": [
        "#@title Import libraries\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import cv2 as cv\n",
        " \n",
        "import matplotlib.pyplot as plt\n",
        " \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import cv2\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.datasets import VOCSegmentation, SBDataset\n",
        "from torchvision.datasets.vision import StandardTransform\n",
        "from torchvision.utils import make_grid\n",
        "from torchvision import transforms\n",
        "from torchvision.models.vgg import VGG, vgg16, make_layers\n",
        "from torch.optim import SGD\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from math import log10\n",
        "from torch.autograd import Variable\n",
        "from easydict import EasyDict as edict\n",
        " \n",
        "!pip install pydensecrf\n",
        " \n",
        "import pydensecrf.densecrf as dcrf\n",
        "import pydensecrf.utils as utils\n",
        "\n",
        "from skimage.measure import compare_ssim\n",
        "from skimage.measure import compare_psnr\n",
        " \n",
        "!nvidia-smi\n",
        "!pip install --extra-index-url https://developer.download.nvidia.com/compute/redist nvidia-dali-cuda100\n",
        " \n",
        "from nvidia.dali.pipeline import Pipeline\n",
        "import nvidia.dali.ops as ops\n",
        "import nvidia.dali.types as types\n",
        "\n",
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "import matplotlib.gridspec as gridspec"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pydensecrf\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/31/5a/1c2ab48e8019d282c128bc5c621332267bb954d32eecdda3ba57306b1551/pydensecrf-1.0rc3.tar.gz (1.0MB)\n",
            "\r\u001b[K     |▎                               | 10kB 15.6MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 13.2MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 12.2MB/s eta 0:00:01\r\u001b[K     |█▎                              | 40kB 8.6MB/s eta 0:00:01\r\u001b[K     |█▋                              | 51kB 10.3MB/s eta 0:00:01\r\u001b[K     |██                              | 61kB 12.0MB/s eta 0:00:01\r\u001b[K     |██▏                             | 71kB 10.8MB/s eta 0:00:01\r\u001b[K     |██▌                             | 81kB 12.0MB/s eta 0:00:01\r\u001b[K     |██▉                             | 92kB 12.0MB/s eta 0:00:01\r\u001b[K     |███▏                            | 102kB 11.5MB/s eta 0:00:01\r\u001b[K     |███▌                            | 112kB 11.5MB/s eta 0:00:01\r\u001b[K     |███▉                            | 122kB 11.5MB/s eta 0:00:01\r\u001b[K     |████                            | 133kB 11.5MB/s eta 0:00:01\r\u001b[K     |████▍                           | 143kB 11.5MB/s eta 0:00:01\r\u001b[K     |████▊                           | 153kB 11.5MB/s eta 0:00:01\r\u001b[K     |█████                           | 163kB 11.5MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 174kB 11.5MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 184kB 11.5MB/s eta 0:00:01\r\u001b[K     |██████                          | 194kB 11.5MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 204kB 11.5MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 215kB 11.5MB/s eta 0:00:01\r\u001b[K     |███████                         | 225kB 11.5MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 235kB 11.5MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 245kB 11.5MB/s eta 0:00:01\r\u001b[K     |████████                        | 256kB 11.5MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 266kB 11.5MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 276kB 11.5MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 286kB 11.5MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 296kB 11.5MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 307kB 11.5MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 317kB 11.5MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 327kB 11.5MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 337kB 11.5MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 348kB 11.5MB/s eta 0:00:01\r\u001b[K     |███████████                     | 358kB 11.5MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 368kB 11.5MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 378kB 11.5MB/s eta 0:00:01\r\u001b[K     |████████████                    | 389kB 11.5MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 399kB 11.5MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 409kB 11.5MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 419kB 11.5MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 430kB 11.5MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 440kB 11.5MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 450kB 11.5MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 460kB 11.5MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 471kB 11.5MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 481kB 11.5MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 491kB 11.5MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 501kB 11.5MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 512kB 11.5MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 522kB 11.5MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 532kB 11.5MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 542kB 11.5MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 552kB 11.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 563kB 11.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 573kB 11.5MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 583kB 11.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 593kB 11.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 604kB 11.5MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 614kB 11.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 624kB 11.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 634kB 11.5MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 645kB 11.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 655kB 11.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 665kB 11.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 675kB 11.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 686kB 11.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 696kB 11.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 706kB 11.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 716kB 11.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 727kB 11.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 737kB 11.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 747kB 11.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 757kB 11.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 768kB 11.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 778kB 11.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 788kB 11.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 798kB 11.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 808kB 11.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 819kB 11.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 829kB 11.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 839kB 11.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 849kB 11.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 860kB 11.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 870kB 11.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 880kB 11.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 890kB 11.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 901kB 11.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 911kB 11.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 921kB 11.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 931kB 11.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 942kB 11.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 952kB 11.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 962kB 11.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 972kB 11.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 983kB 11.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 993kB 11.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.0MB 11.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.0MB 11.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.0MB 11.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.0MB 11.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.0MB 11.5MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pydensecrf\n",
            "  Building wheel for pydensecrf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pydensecrf: filename=pydensecrf-1.0rc3-cp36-cp36m-linux_x86_64.whl size=2153725 sha256=23ee9a78ba291d39f72a1b1e8165f7ee7be2ef51c184b7a5e126f147d9046f01\n",
            "  Stored in directory: /root/.cache/pip/wheels/92/6f/ec/5c49c25de8c42c872de50ff53582ba3ead850ce52a81e73ac7\n",
            "Successfully built pydensecrf\n",
            "Installing collected packages: pydensecrf\n",
            "Successfully installed pydensecrf-1.0rc3\n",
            "Thu Dec 10 14:08:30 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.45.01    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0    24W / 300W |      0MiB / 16130MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "Looking in indexes: https://pypi.org/simple, https://developer.download.nvidia.com/compute/redist\n",
            "Collecting nvidia-dali-cuda100\n",
            "\u001b[?25l  Downloading https://developer.download.nvidia.com/compute/redist/nvidia-dali-cuda100/nvidia_dali_cuda100-0.28.0-1761993-py3-none-manylinux2014_x86_64.whl (367.9MB)\n",
            "\u001b[K     |████████████████████████████████| 367.9MB 38kB/s \n",
            "\u001b[?25hInstalling collected packages: nvidia-dali-cuda100\n",
            "Successfully installed nvidia-dali-cuda100-0.28.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "TZgci8gkRSyv"
      },
      "source": [
        "#@title Setting parameters\n",
        "\n",
        "torch.manual_seed(470)\n",
        "torch.cuda.manual_seed(470)\n",
        " \n",
        "args = edict()\n",
        " \n",
        "args.batch_size = 1\n",
        "args.lr = 1e-4\n",
        "args.momentum = 0.9\n",
        "args.weight_decay = 5e-4\n",
        "args.epoch = 10\n",
        "args.tensorboard = True\n",
        "args.gpu = True\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() and args.gpu else 'cpu'\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIRFpjlgbdAT"
      },
      "source": [
        "#@title Create directory name.\r\n",
        "\r\n",
        "result_dir = Path(root) / 'results'\r\n",
        "result_dir.mkdir(parents=True, exist_ok=True)\r\n",
        "\r\n",
        "data_dir = Path(root) / 'data'\r\n",
        "data_dir.mkdir(parents=True, exist_ok=True)\r\n",
        "\r\n",
        "train_dir = data_dir / 'test_data'\r\n",
        "valid_dir = data_dir / 'valid_data'\r\n",
        "\r\n",
        "train_dir.mkdir(parents=True, exist_ok=True)\r\n",
        "valid_dir.mkdir(parents=True, exist_ok=True)\r\n",
        "\r\n",
        "num_trial=0\r\n",
        "\r\n",
        "# Modify parent_dir here if you want to resume from a , or to rename directory.\r\n",
        "# parent_dir = result_dir / 'trial_99'\r\n",
        "\r\n",
        "parent_dir = result_dir / f'trial_{num_trial}'\r\n",
        "while parent_dir.is_dir():\r\n",
        "    num_trial = int(parent_dir.name.replace('trial_',''))\r\n",
        "    parent_dir = result_dir / f'trial_{num_trial+1}'\r\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "9q9GHGLFnJZ7"
      },
      "source": [
        "#@title Helper functions\n",
        "\n",
        "normalize = transforms.Normalize(mean = [0.485, 0.456, 0.406],std = [0.229, 0.224, 0.225])\n",
        "unnormalize = transforms.Normalize(mean = [-2.118, -2.036, -1.804], std = [4.367, 4.464, 4.444])\n",
        "\n",
        "def crop(tensor,top,left,h,w):\n",
        "    return tensor[:,:,top:top+h,left:left+w]\n",
        "\n",
        "def optical_flow_warp(image, image_optical_flow):\n",
        "\n",
        "    b, _ , h, w = image.size()\n",
        "\n",
        "    grid = np.meshgrid(range(w), range(h))\n",
        "    grid = np.stack(grid, axis=-1).astype(np.float64)\n",
        "    grid[:, :, 0] = grid[:, :, 0] * 2 / (w - 1) -1\n",
        "    grid[:, :, 1] = grid[:, :, 1] * 2 / (h - 1) -1\n",
        "    grid = grid.transpose(2, 0, 1)\n",
        "    grid = np.tile(grid, (b, 1, 1, 1))\n",
        "    grid = Variable(torch.Tensor(grid))\n",
        "    if image_optical_flow.is_cuda == True:\n",
        "        grid = grid.cuda()\n",
        "\n",
        "    flow_0 = torch.unsqueeze(image_optical_flow[:, 0, :, :] * 31 / (w - 1), dim=1)\n",
        "    flow_1 = torch.unsqueeze(image_optical_flow[:, 1, :, :] * 31 / (h - 1), dim=1)\n",
        "\n",
        "    grid = grid + torch.cat((flow_0, flow_1),1)\n",
        "    grid = grid.transpose(1, 2)\n",
        "    grid = grid.transpose(3, 2)\n",
        "\n",
        "    output = F.grid_sample(image, grid, padding_mode='border')\n",
        "    return output\n",
        "\n",
        "def space_to_depth(x, block_size):\n",
        "    n, c, h, w = x.size()\n",
        "    unfolded_x = torch.nn.functional.unfold(x, block_size, stride=block_size)\n",
        "    return unfolded_x.view(n, c * block_size ** 2, h // block_size, w // block_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siYvQgXQok3Q"
      },
      "source": [
        "# Tensorboard Settings\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbBl6brJopBK"
      },
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import datetime\n",
        "\n",
        "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "\n",
        "writer = SummaryWriter(logdir)\n",
        "\n",
        "print()\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1iZWv-Fk-Bh"
      },
      "source": [
        "# 2. Define Networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "_kOaLexbitsA"
      },
      "source": [
        "#@title FlowNet\n",
        "# https://github.com/amanchadha/FRVSR-GAN/blob/master/FRVSRGAN_Models.py\n",
        "class ConvLeaky(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim):\n",
        "        super(ConvLeaky, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=in_dim, out_channels=out_dim,\n",
        "                               kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=out_dim, out_channels=out_dim,\n",
        "                               kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        out = self.conv1(input)\n",
        "        out = F.leaky_relu(out, 0.2)\n",
        "        out = self.conv2(out)\n",
        "        out = F.leaky_relu(out, 0.2)\n",
        "        return out\n",
        "\n",
        "class FNetBlock(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, typ):\n",
        "        super(FNetBlock, self).__init__()\n",
        "        self.convleaky = ConvLeaky(in_dim, out_dim)\n",
        "        if typ == \"maxpool\":\n",
        "            self.final = lambda x: F.max_pool2d(x, kernel_size=2, ceil_mode=True)\n",
        "        elif typ == \"bilinear\":\n",
        "            self.final = lambda x: F.interpolate(x, scale_factor=2, mode=\"bilinear\")\n",
        "        else:\n",
        "            raise Exception('Type does not match any of maxpool or bilinear')\n",
        "\n",
        "    def forward(self, input):\n",
        "        out = self.convleaky(input)\n",
        "        out = self.final(out)\n",
        "        return out\n",
        "\n",
        "# similar as FNet of FRVSR from https://arxiv.org/pdf/1801.04590.pdf\n",
        "class FNet(nn.Module):\n",
        "    def __init__(self, in_dim=9):\n",
        "        super(FNet, self).__init__()\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            FNetBlock(in_dim, 32, typ=\"maxpool\",),\n",
        "            FNetBlock(32, 64, typ=\"maxpool\"),\n",
        "            FNetBlock(64, 128, typ=\"maxpool\")\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            FNetBlock(128, 256, typ=\"bilinear\"),\n",
        "            FNetBlock(256, 128, typ=\"bilinear\"),\n",
        "            FNetBlock(128, 64, typ=\"bilinear\")\n",
        "        )\n",
        "        self.conv1 = nn.Conv2d(in_channels=64, out_channels=32,\n",
        "                               kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=2,\n",
        "                               kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        _, _ , tmp_x, tmp_y = input.size()\n",
        "\n",
        "        output = self.encoder(input)\n",
        "        output = self.decoder(output) \n",
        "        output = crop(output,0,0, tmp_x, tmp_y)\n",
        "        output = self.conv1(output)\n",
        "        output = F.leaky_relu(output, 0.2)\n",
        "        output = self.conv2(output)\n",
        "        output = torch.tanh(output)\n",
        "        \n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "gr8bHKjelty4"
      },
      "source": [
        "#@title Discriminator Net \n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Discriminator,self).__init__()\n",
        "    self.l_relu = nn.LeakyReLU()\n",
        "    self.outer_conv = nn.Conv2d(3,64,3)\n",
        "\n",
        "    self.conv_64c = nn.Conv2d(64,64,3,2)\n",
        "    self.bn_64c = nn.BatchNorm2d(64)\n",
        "\n",
        "    self.conv_128e = nn.Conv2d(64,128,3,1)\n",
        "    self.bn_128e = nn.BatchNorm2d(128)\n",
        "\n",
        "    self.conv_128c = nn.Conv2d(128,128,3,2)\n",
        "    self.bn_128c = nn.BatchNorm2d(128)\n",
        "\n",
        "    self.conv_256e = nn.Conv2d(128,256,3,1)\n",
        "    self.bn_256e = nn.BatchNorm2d(256)\n",
        "\n",
        "    self.conv_256c = nn.Conv2d(256,256,3,2)\n",
        "    self.bn_256c = nn.BatchNorm2d(256)\n",
        "\n",
        "    self.conv_512e = nn.Conv2d(256,512,3,1)\n",
        "    self.bn_512e = nn.BatchNorm2d(512)\n",
        "\n",
        "    self.conv_512c = nn.Conv2d(512,512,3,2)\n",
        "    self.bn_512c = nn.BatchNorm2d(512)\n",
        "\n",
        "    self.flatten = nn.AdaptiveAvgPool2d(1)\n",
        "    self.lin1 = nn.Conv2d(512, 1024, kernel_size=1)\n",
        "    self.lin2 = nn.Conv2d(1024, 1, kernel_size=1)\n",
        "    self.sig = nn.Sigmoid()\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.l_relu(self.outer_conv(x))\n",
        "    x = self.l_relu(self.bn_64c(self.conv_64c(x)))\n",
        "    x = self.l_relu(self.bn_128e(self.conv_128e(x)))\n",
        "    x = self.l_relu(self.bn_128c(self.conv_128c(x)))\n",
        "    x = self.l_relu(self.bn_256e(self.conv_256e(x)))\n",
        "    x = self.l_relu(self.bn_256c(self.conv_256c(x)))\n",
        "    x = self.l_relu(self.bn_512e(self.conv_512e(x)))\n",
        "    x = self.l_relu(self.bn_512c(self.conv_512c(x)))\n",
        "\n",
        "    x = self.flatten(x)\n",
        "    x = self.l_relu(self.lin1(x))\n",
        "    x = self.sig(self.lin2(x))\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "_7D05O-j7mP7"
      },
      "source": [
        "#@title SR Net\n",
        "\n",
        "class Empty_Model(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(Empty_Model, self).__init__()\n",
        "      self.conv = nn.ConvTranspose2d(3,3,7,2,bias = False)\n",
        "\n",
        "    def forward(self, x):\n",
        "      shape = x.shape\n",
        "      img2 = self.conv(x) \n",
        "\n",
        "      return crop(img2,2,2,shape[2] * 2, shape[3] * 3)\n",
        "\n",
        "class Residual_Block(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "      super(Residual_Block, self).__init__()\n",
        "      self.conv1 = nn.Conv2d(64,64,3,padding=1)\n",
        "      self.conv2 = nn.Conv2d(64,64,3,padding=1)\n",
        "      self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self,x):\n",
        "      rc = x\n",
        "      x = self.conv1(x)\n",
        "      x = self.relu(x)\n",
        "      x = self.conv2(x)\n",
        "      \n",
        "      x = x + 0.1 * rc\n",
        "      return x\n",
        "\n",
        "class SRNet(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(SRNet, self).__init__()\n",
        "      self.conv_init = nn.Conv2d(3,64,3,padding = 1)\n",
        "      self.relu = nn.ReLU()\n",
        "\n",
        "      self.res_thru = nn.Sequential(\n",
        "                Residual_Block(),\n",
        "                Residual_Block(),\n",
        "                Residual_Block(),\n",
        "                Residual_Block(),\n",
        "                Residual_Block(),\n",
        "                Residual_Block(),\n",
        "                Residual_Block(),\n",
        "                Residual_Block(),\n",
        "                Residual_Block(),\n",
        "                Residual_Block()\n",
        "              )\n",
        "      self.ct1 = nn.ConvTranspose2d(64,64,3,2)\n",
        "\n",
        "      self.conv_img = nn.Conv2d(64,3,3,padding = 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = x[1]\n",
        "      shape = x.shape\n",
        "\n",
        "      x = self.conv_init(x) \n",
        "      x = self.relu(x)\n",
        "\n",
        "      residual = x\n",
        "\n",
        "      x = self.res_thru(x)\n",
        "      \n",
        "      x += residual\n",
        "\n",
        "      x = self.ct1(x)\n",
        "      x = self.relu(x)\n",
        "\n",
        "      x = self.conv_img(x)\n",
        "\n",
        "      return crop(x,1,1,shape[2] * 2,shape[3] * 2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "sinHlpirmt4-"
      },
      "source": [
        "#@title SRFlowNet\n",
        "class SRFlowNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(SRFlowNet, self).__init__()\n",
        "    self.sr_net = SRNet()\n",
        "\n",
        "    self.conv_init = nn.Conv2d(6,64,3,padding = 1)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.fnet = FNet()\n",
        "\n",
        "    self.res_thru = nn.Sequential(\n",
        "              Residual_Block(),\n",
        "              Residual_Block(),\n",
        "              Residual_Block(),\n",
        "              Residual_Block(),\n",
        "              Residual_Block(),\n",
        "              Residual_Block(),\n",
        "              Residual_Block(),\n",
        "              Residual_Block(),\n",
        "              Residual_Block(),\n",
        "              Residual_Block()\n",
        "            )\n",
        "    self.ct1 = nn.ConvTranspose2d(64,64,3,2)\n",
        "    self.conv_skip = nn.Conv2d(3,64,3,padding = 1)\n",
        "\n",
        "    self.conv_img = nn.Conv2d(64,3,3,padding = 1)\n",
        "\n",
        "  def forward(self, input):\n",
        "    _, x, _ = input\n",
        "    f_x = torch.cat(input, dim=1)\n",
        "\n",
        "    curr = x\n",
        "    fs = self.fnet(f_x)\n",
        "\n",
        "    shape = x.shape\n",
        "    x_warp = optical_flow_warp(curr, fs)\n",
        "    x = torch.cat((x, x_warp), dim=1)\n",
        "    x = self.conv_init(x) \n",
        "    x = self.relu(x)\n",
        "    residual = x\n",
        "    x = self.res_thru(x)\n",
        "    x += residual\n",
        "    x = self.ct1(x)\n",
        "    x= crop(x,1,1,shape[2] * 2,shape[3] * 2)\n",
        "    x += self.conv_skip(F.interpolate(curr, scale_factor=2, mode=\"bilinear\"))\n",
        "    x = self.relu(x)\n",
        "    x = self.conv_img(x)\n",
        "\n",
        "    return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sl0ZWnn0cTCP"
      },
      "source": [
        "#@title Perceptual Loss\n",
        "# https://github.com/dongheehand/SRGAN-PyTorch/blob/master/losses.py \n",
        "class perceptual_loss(nn.Module):\n",
        "    def __init__(self, vgg):\n",
        "        super(perceptual_loss, self).__init__()\n",
        "\n",
        "        self.vgg = vgg\n",
        "        self.criterion = nn.MSELoss()\n",
        "    def forward(self, HR, SR, layer = 'relu5_4'):\n",
        "        \n",
        "        hr_feat = self.vgg(HR)\n",
        "        sr_feat = self.vgg(SR)\n",
        "        \n",
        "        return self.criterion(hr_feat, sr_feat), hr_feat, sr_feat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5L-JNv1gMtD"
      },
      "source": [
        "NVIDIA DALI dataloader pipeline configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "G-e3DuBzgM3C"
      },
      "source": [
        "#@title DALI video pipline config\n",
        "batch_size=3\n",
        "sequence_length=3\n",
        "\n",
        "initial_prefetch_size=16\n",
        "\n",
        "video_directory = os.path.join(train_dir)\n",
        "valid_directory = os.path.join(valid_dir)\n",
        "\n",
        "video_files = [video_directory + '/' + f for f in os.listdir(video_directory)]\n",
        "valid_files = [valid_directory + '/' + f for f in os.listdir(valid_directory)]\n",
        "\n",
        "shuffle=True\n",
        "args.batch_size = batch_size\n",
        "\n",
        "class VideoPipe(Pipeline):\n",
        "    def __init__(self, batch_size, num_threads, device_id, data, shuffle):\n",
        "        super(VideoPipe, self).__init__(batch_size, num_threads, device_id, seed=16)\n",
        "        self.input = ops.VideoReader(device=\"gpu\", filenames=data, sequence_length=sequence_length,\n",
        "                                     shard_id=0, num_shards=1,\n",
        "                                     random_shuffle=shuffle, initial_fill=initial_prefetch_size,pad_last_batch = True)\n",
        "\n",
        "    def define_graph(self):\n",
        "        output = self.input(name=\"Reader\")\n",
        "        return output\n",
        "\n",
        "pipe = VideoPipe(batch_size=batch_size, num_threads=2, device_id=0, data=video_files, shuffle=shuffle)\n",
        "pipe.build()\n",
        "\n",
        "valid_pipe = VideoPipe(batch_size=1, num_threads=2, device_id=0, data=valid_files, shuffle=False)\n",
        "valid_pipe.build()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtJIDIE3aMaY"
      },
      "source": [
        "# Training Phase"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZxTmuX2b8kq"
      },
      "source": [
        "GAN training Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "LFyt7wUcSHtn"
      },
      "source": [
        "#@title GAN training code\n",
        "\n",
        "def test_net_valid(generator,epoch_num):\n",
        "    randcrop = torch.nn.Sequential(\n",
        "        torchvision.transforms.RandomCrop(400, padding=None, pad_if_needed=False, fill=0, padding_mode='constant'),\n",
        "        )\n",
        "    generator.eval()\n",
        "\n",
        "    test_iter = 10\n",
        "    \n",
        "    avg_psnr = 0\n",
        "    avg_ssim = 0\n",
        "\n",
        "    for i in range(test_iter):\n",
        "      pipe_out = valid_pipe.run() \n",
        "      pipe_out = pipe_out[0].as_cpu().as_array()\n",
        "      pipe_out = randcrop(torch.from_numpy(pipe_out).permute(0,1,4,2,3))\n",
        "\n",
        "      frame_list = torch.chunk(pipe_out,chunks = pipe_out.shape[1],dim=1) \n",
        "\n",
        "      prev_frame =  frame_list[0].squeeze(1).to(device).float()/255.0\n",
        "      frame =       frame_list[1].squeeze(1).float()/255.0         \n",
        "      next_frame =  frame_list[2].squeeze(1).to(device).float()/255.0\n",
        "\n",
        "      orig_img = frame.squeeze(0).transpose(0,2).transpose(0,1).cpu().detach().numpy()\n",
        "\n",
        "      h = frame.shape[2]\n",
        "      w = frame.shape[3]\n",
        "\n",
        "      prev_lq =   torch.nn.functional.interpolate(prev_frame,size=(h//2,w//2))\n",
        "      lq_frame =  torch.nn.functional.interpolate(frame,size=(h//2,w//2))\n",
        "      next_lq =   torch.nn.functional.interpolate(next_frame,size=(h//2,w//2))\n",
        "  \n",
        "      prev_frame =  normalize(prev_frame).to(device)\n",
        "      frame     =   normalize(frame).to(device)\n",
        "      next_frame =  normalize(next_frame).to(device)\n",
        "\n",
        "      prev_lq =  normalize(prev_lq).to(device)\n",
        "      lq_frame  =   normalize(lq_frame).to(device)\n",
        "      next_lq =  normalize(next_lq).to(device)\n",
        "\n",
        "      frame_recon = generator((prev_lq,lq_frame,next_lq))\n",
        "      n_image = unnormalize(frame_recon)\n",
        "      n_image = n_image.squeeze(0).transpose(0,2).transpose(0,1).cpu().detach().numpy()\n",
        "\n",
        "      psnr = compare_psnr(orig_img,n_image)\n",
        "\n",
        "      if i == 0:\n",
        "        ssim = compare_ssim(orig_img,n_image,win_size = 11,multichannel=True)\n",
        "        avg_ssim = ssim\n",
        "      avg_psnr += psnr/test_iter\n",
        "      \n",
        "    print(f\"Validation set testing: psnr = {avg_psnr:.5f}, ssim = {avg_ssim:.5f}\")\n",
        "    writer.add_scalar(\"PSNR/VALID\",  avg_psnr, epoch_num)\n",
        "    writer.add_scalar(\"SSIM/VALID\",  avg_ssim, epoch_num)\n",
        "\n",
        "\n",
        "def train_net_GAN(net,discriminator, resume,n_iter,n_iter_desc,gen_epoc,dis_epoch,load_optim = False,fname = None):\n",
        "    criterion = nn.MSELoss().to(device) \n",
        "\n",
        "    randcrop = torch.nn.Sequential(\n",
        "        torchvision.transforms.RandomCrop(400, padding=None, pad_if_needed=False, fill=0, padding_mode='constant'),\n",
        "        )\n",
        "\n",
        "    epoch = 0\n",
        "\n",
        "    optimizer = torch.optim.Adam(net.parameters(),lr=args.lr)\n",
        "    net.train()\n",
        " \n",
        "    while epoch < gen_epoch: \n",
        "        if gen_epoch == 0:\n",
        "          continue\n",
        "        t1 = time.time()\n",
        "        first = True\n",
        "        loss_total = 0\n",
        "\n",
        "        for i in range(n_iter):\n",
        "            pipe_out = pipe.run() \n",
        "            pipe_out = pipe_out[0].as_cpu().as_array()\n",
        "            pipe_out = randcrop(torch.from_numpy(pipe_out).permute(0,1,4,2,3))\n",
        "\n",
        "            frame_list = torch.chunk(pipe_out,chunks = pipe_out.shape[1],dim=1) \n",
        "\n",
        "            prev_frame =  frame_list[0].squeeze(1).to(device).float()/255.0\n",
        "            frame =       frame_list[1].squeeze(1).float()/255.0   \n",
        "            next_frame =  frame_list[2].squeeze(1).to(device).float()/255.0\n",
        " \n",
        "            h = frame.shape[2]\n",
        "            w = frame.shape[3]\n",
        " \n",
        "            prev_lq =   torch.nn.functional.interpolate(prev_frame,size=(h//2,w//2))\n",
        "            lq_frame =  torch.nn.functional.interpolate(frame,size=(h//2,w//2))\n",
        "            next_lq =   torch.nn.functional.interpolate(next_frame,size=(h//2,w//2))\n",
        " \n",
        "        \n",
        "            prev_frame =  normalize(prev_frame).to(device)\n",
        "            frame     =   normalize(frame).to(device)\n",
        "            next_frame =  normalize(next_frame).to(device)\n",
        " \n",
        "            prev_lq =  normalize(prev_lq).to(device)\n",
        "            lq_frame  =   normalize(lq_frame).to(device)\n",
        "            next_lq =  normalize(next_lq).to(device)\n",
        " \n",
        " \n",
        "            frame_recon = net((prev_lq,lq_frame,next_lq))\n",
        " \n",
        "            loss = criterion(frame,frame_recon)\n",
        " \n",
        "            net.zero_grad()\n",
        " \n",
        "            loss.backward()\n",
        " \n",
        "            optimizer.step()\n",
        "            \n",
        "            if i % 10 == 0:\n",
        "              writer.add_scalar(\"gen-only-loss/train\", loss, epoch*n_iter + i)\n",
        " \n",
        "            loss_total += loss.item()/n_iter\n",
        "\n",
        "        t = time.time() - t1\n",
        "        \n",
        "        print(\"--epoch finished-- \" + str(epoch) + \" time taken: \" + str(t))\n",
        "        print(f'train loss : {loss_total:.5f}')\n",
        "        test_net_valid(net,epoch)\n",
        "\n",
        "        torch.save(net.state_dict(),  result_dir / f'tmp_{type(net).__name__}_.ckpt')\n",
        "        torch.save(optimizer.state_dict(),  result_dir / f'tmp_{type(optimizer).__name__}_.ckpt')\n",
        "        epoch += 1\n",
        "\n",
        "        writer.flush()\n",
        "\n",
        "    vgg_net = torchvision.models.vgg19(pretrained=True).to(device)\n",
        "    vgg_net.eval()\n",
        "    VGG_loss = perceptual_loss(vgg_net)\n",
        "    cross_ent = nn.BCELoss()\n",
        " \n",
        "    real_label = torch.ones((batch_size, 1,1,1)).to(device)\n",
        "    fake_label = torch.zeros((batch_size, 1,1,1)).to(device)\n",
        " \n",
        "    ce_loss = nn.BCELoss()\n",
        " \n",
        "    discriminator.train()\n",
        "\n",
        "    optimizer = torch.optim.Adam(net.parameters(),lr=args.lr * 0.1)\n",
        "    disc_optimizer = torch.optim.Adam(discriminator.parameters(),lr=args.lr * 0.1)\n",
        "\n",
        "    if load_optim:\n",
        "      optimizer.load_state_dict(fname)\n",
        "      disc_optimizer.load_state_dict(fname)\n",
        "\n",
        "    epoch = 0\n",
        "    torch.save(net.state_dict(),  result_dir / f'disc_{type(net).__name__}_.ckpt')\n",
        "\n",
        "    print(\"TRAINING DISCRIMINATOR NETWORK\")\n",
        "    while epoch < dis_epoch: \n",
        "        t1 = time.time()\n",
        "        first = True\n",
        "        loss_total = 0\n",
        "        L2_Loss = 0\n",
        "        adv_Loss = 0\n",
        "        disc_loss_total = 0\n",
        " \n",
        "        for i in range(n_iter_desc):\n",
        "            pipe_out = pipe.run()\n",
        "            pipe_out = pipe_out[0].as_cpu().as_array()\n",
        "            pipe_out = randcrop(torch.from_numpy(pipe_out).permute(0,1,4,2,3))\n",
        " \n",
        "            frame_list = torch.chunk(pipe_out,chunks = pipe_out.shape[1],dim=1) \n",
        " \n",
        "            prev_frame =  frame_list[0].squeeze(1).to(device).float()/255.0\n",
        "            frame =       frame_list[1].squeeze(1).float()/255.0 \n",
        "            next_frame =  frame_list[2].squeeze(1).to(device).float()/255.0\n",
        " \n",
        "            h = frame.shape[2]\n",
        "            w = frame.shape[3]\n",
        " \n",
        "            prev_lq =   torch.nn.functional.interpolate(prev_frame,size=(h//2,w//2))\n",
        "            lq_frame =  torch.nn.functional.interpolate(frame,size=(h//2,w//2))\n",
        "            next_lq =   torch.nn.functional.interpolate(next_frame,size=(h//2,w//2))\n",
        "\n",
        "            prev_frame =  normalize(prev_frame).to(device)\n",
        "            frame     =   normalize(frame).to(device)\n",
        "            next_frame =  normalize(next_frame).to(device)\n",
        " \n",
        "            prev_lq =  normalize(prev_lq).to(device)\n",
        "            lq_frame  =   normalize(lq_frame).to(device)\n",
        "            next_lq =  normalize(next_lq).to(device)\n",
        "\n",
        "            frame = Variable(frame)\n",
        " \n",
        "            frame_recon = net((Variable(prev_lq),Variable(lq_frame),Variable(next_lq)))\n",
        "\n",
        "            discriminator.zero_grad() \n",
        "            \n",
        "            fake_prob = discriminator(frame_recon)\n",
        "            real_prob = discriminator(frame)\n",
        "\n",
        "            fake_error = ce_loss(fake_prob,fake_label) \n",
        "            real_error = ce_loss(real_prob,real_label) \n",
        " \n",
        "            disc_loss = fake_error + real_error\n",
        "\n",
        "            disc_loss.backward(retain_graph = True) \n",
        "\n",
        "            disc_optimizer.step()  \n",
        "\n",
        "            net.zero_grad()    \n",
        " \n",
        "            percep_loss, hr_feat, sr_feat = VGG_loss((frame + 1.0) / 2.0, (frame_recon + 1.0) / 2.0, layer = 'relu5_4') \n",
        "            L2loss = criterion(frame,frame_recon)\n",
        " \n",
        "            percep_loss *= 0.006 \n",
        "            adversarial_loss = cross_ent(discriminator(frame_recon), real_label) * 1e-3\n",
        " \n",
        "            total_loss = percep_loss + adversarial_loss + L2loss\n",
        " \n",
        "            total_loss.backward() \n",
        "            \n",
        "            optimizer.step() \n",
        "\n",
        "            if i % 10 == 0:\n",
        "              writer.add_scalar(\"adv-total_loss/train\",         total_loss, epoch*n_iter_desc + i)\n",
        "              writer.add_scalar(\"adv-adversarial_loss/train\",   adversarial_loss, epoch*n_iter_desc + i)\n",
        "              writer.add_scalar(\"adv-L2loss/train\",             L2loss, epoch*n_iter_desc + i)\n",
        "              writer.add_scalar(\"adv-disc_loss/train\",          disc_loss, epoch*n_iter_desc + i)\n",
        " \n",
        "            loss_total += total_loss.item()/n_iter_desc\n",
        "            adv_Loss += adversarial_loss.item()/n_iter_desc\n",
        "            L2_Loss += L2loss.item()/n_iter_desc\n",
        "            disc_loss_total += disc_loss.item()/n_iter_desc\n",
        "\n",
        "        t = time.time() - t1\n",
        "        \n",
        " \n",
        "        epoch += 1\n",
        "        torch.save(net.state_dict(),  result_dir / f'tmp_{type(net).__name__}_.ckpt')\n",
        "        torch.save(discriminator.state_dict(),  result_dir / f'tmp_{type(discriminator).__name__}_.ckpt')\n",
        "        torch.save(optimizer.state_dict(),  result_dir / f'tmp_{type(optimizer).__name__}_.ckpt')\n",
        "        torch.save(disc_optimizer.state_dict(),  result_dir / f'tmp_dc_{type(disc_optimizer).__name__}_.ckpt')\n",
        "\n",
        "        print(\"--fine grain epoch finished-- \" + str(epoch-1) + \" time taken: \" + str(t))\n",
        "        print(f'loss : {loss_total:.5f} ,adv_loss :{adv_Loss:.5f},l2_loss: {L2_Loss:.5f} disc_loss :{disc_loss_total:.5f}')\n",
        "        test_net_valid(net,epoch-1)\n",
        "        writer.flush()\n",
        "\n",
        "    torch.save(net.state_dict(),  result_dir / f'Attempt_2.ckpt')\n",
        "    torch.save(discriminator.state_dict(),  result_dir / f'disc_Attempt_2.ckpt')\n",
        "\n",
        "    torch.save(optimizer.state_dict(),  result_dir / f'disc_{type(optimizer).__name__}_.ckpt')\n",
        "    torch.save(disc_optimizer.state_dict(),  result_dir / f'disc_dc_{type(disc_optimizer).__name__}_.ckpt')\n",
        "    \n",
        "    writer.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "DU6HmIy4Q_Cy"
      },
      "source": [
        "#@title Train Model\n",
        "\n",
        "#Set cont as True if you want to continue the training of gernerator based on existing ckpt\n",
        "#load_disc is same as cont but for discriminator\n",
        "\n",
        "#Put generator's file name and discriminator's file name in disc_file and gen_file.\n",
        "#ex) disc_file = 'disc_Attempt_2.ckpt'\n",
        "#ex) gen_file = 'Attempt_2.ckpt'\n",
        "\n",
        "#Set number of epochs and iterations per epoch. ex) gen_epoch=5, gen_iter=100, dis_epoch = 12, dis_iter = 100\n",
        "\n",
        "###########################\n",
        "cont = False\n",
        "load_disc = False\n",
        "\n",
        "disc_file = ''\n",
        "gen_file = ''\n",
        " \n",
        "gen_epoch =\n",
        "gen_iter =\n",
        " \n",
        "dis_epoch = \n",
        "dis_iter =\n",
        "###########################\n",
        " \n",
        "if cont:\n",
        "\n",
        "  model = SRFlowNet().to(device)\n",
        "  desc = Discriminator().to(device)\n",
        " \n",
        "  model.load_state_dict(torch.load(result_dir / gen_file))\n",
        "  if load_disc:\n",
        "    desc.load_state_dict(torch.load(result_dir / disc_file))\n",
        "  train_net_GAN(model,desc,False,gen_iter,dis_iter,gen_epoch,dis_epoch)\n",
        " \n",
        "else:\n",
        "  model = SRFlowNet().to(device)\n",
        "  desc = Discriminator().to(device)\n",
        " \n",
        "  if load_disc:\n",
        "    desc.load_state_dict(torch.load(result_dir / disc_file))\n",
        " \n",
        "  train_net_GAN(model,desc,False,gen_iter,dis_iter,gen_epoch,dis_epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUso5OBCa0SS"
      },
      "source": [
        "# Model Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhViSRd_fXV3",
        "cellView": "code",
        "outputId": "dbf81347-99a8-47f9-cedd-96e4f3a1cb6f"
      },
      "source": [
        "#@title Model Test\n",
        "#We checked the place that you should fill with ###########################\n",
        "#imdir is the path for input images.\n",
        "#Since we put 3 images, you should fill imdir1, imdir2, imdir3.\n",
        "\n",
        "#svdir is path for saving result.\n",
        "#ex) svdir = Path(root) / 'valid_test' / 'nalckiep.png'\n",
        "\n",
        "model_file = 'Attempt_2.ckpt'\n",
        "model = SRFlowNet().to(device)\n",
        "model.load_state_dict(torch.load(result_dir / model_file))\n",
        "model.eval()\n",
        " \n",
        "images = []\n",
        " \n",
        "loader = transforms.Compose([transforms.ToTensor()])\n",
        "###########################\n",
        "imdir1 = \n",
        "###########################\n",
        "image1 = cv.imread(str(imdir1))\n",
        "image1 = loader(image1).float()\n",
        "image1 = image1.unsqueeze(0).to(device)\n",
        "images.append(normalize(image1))\n",
        " \n",
        "loader = transforms.Compose([transforms.ToTensor()])\n",
        "###########################\n",
        "imdir2 =\n",
        "###########################\n",
        "image2 = cv.imread(str(imdir2))\n",
        "image2 = loader(image2).float()\n",
        "image2 = image2.unsqueeze(0).to(device)\n",
        "images.append(normalize(image2))\n",
        " \n",
        "loader = transforms.Compose([transforms.ToTensor()])\n",
        "###########################\n",
        "imdir3 =\n",
        "###########################\n",
        "image3 = cv.imread(str(imdir3))\n",
        "image3 = loader(image3).float()\n",
        "image3 = image3.unsqueeze(0).to(device)\n",
        "images.append(normalize(image3))\n",
        " \n",
        "n_image = model(images)\n",
        "n_image = unnormalize(n_image)\n",
        "  \n",
        "n_image2 = n_image.squeeze(0).transpose(0,2).transpose(0,1).cpu().detach().numpy()\n",
        "n_image2 =(n_image2*255).astype(np.uint8)\n",
        "###########################\n",
        "svdir =\n",
        "###########################\n",
        "cv.imwrite(str(svdir),n_image2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3063: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode))\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3385: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
            "  warnings.warn(\"Default grid_sample and affine_grid behavior has changed \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHk5zvX1Cwj1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}