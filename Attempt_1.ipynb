{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Attempt 1",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jlee335/cs470_Team7_VideoEnhancer/blob/main/Attempt_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HhJ5_lHtlph"
      },
      "source": [
        "##<Attempt 1>\r\n",
        "#####This code is composed with\r\n",
        "\r\n",
        "1.   Basic settings\r\n",
        "2.   Tensorboard Settings\r\n",
        "3.   Define Network\r\n",
        "4.   Training Phase\r\n",
        "5.   Model test\r\n",
        "\r\n",
        "#####You should fill the part surrounded by ##################.\r\n",
        "##### Mostly it is path and model file, training settings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLoUDwtQY4yW"
      },
      "source": [
        "# 1. Basic Settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Of3IYnH0dxb0",
        "cellView": "code"
      },
      "source": [
        "#@title Mount google drive\n",
        "# login with your google account and type authorization code to mount on your google drive.\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        " \n",
        "# Specify the directory path where `Attempt1.ipynb` exists.\n",
        "# ex) root = '/gdrive/My Drive/Final_Project/'\n",
        "###########################\n",
        "root = '/gdrive/My Drive/cs470_Team7_VideoEnhancer/'\n",
        "###########################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "530xKDIOesY9"
      },
      "source": [
        "**Import Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-ZNG0VNesGV",
        "cellView": "code"
      },
      "source": [
        "#@title Import libraries\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        " \n",
        "import matplotlib.pyplot as plt\n",
        " \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import cv2 as cv\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.datasets import VOCSegmentation, SBDataset\n",
        "from torchvision.datasets.vision import StandardTransform\n",
        "from torchvision.utils import make_grid\n",
        "from torchvision import transforms\n",
        "from torchvision.models.vgg import VGG, vgg16, make_layers\n",
        "from torch.optim import SGD\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from math import log10\n",
        "!pip install pydensecrf\n",
        "from torch.autograd import Variable\n",
        "from easydict import EasyDict as edict\n",
        " \n",
        "import pydensecrf.densecrf as dcrf\n",
        "import pydensecrf.utils as utils\n",
        " \n",
        "!nvidia-smi\n",
        "!pip install --extra-index-url https://developer.download.nvidia.com/compute/redist nvidia-dali-cuda100\n",
        " \n",
        "from nvidia.dali.pipeline import Pipeline\n",
        "import nvidia.dali.ops as ops\n",
        "import nvidia.dali.types as types\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "import matplotlib.gridspec as gridspec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZgci8gkRSyv",
        "cellView": "code"
      },
      "source": [
        "#@title Setting parameters\n",
        "\n",
        "torch.manual_seed(470)\n",
        "torch.cuda.manual_seed(470)\n",
        " \n",
        "args = edict()\n",
        " \n",
        "args.batch_size = 1\n",
        "args.lr = 1e-4\n",
        "args.momentum = 0.9\n",
        "args.weight_decay = 5e-4\n",
        "args.epoch = 10\n",
        "args.tensorboard = True\n",
        "args.gpu = True\n",
        "\n",
        " \n",
        "device = 'cuda' if torch.cuda.is_available() and args.gpu else 'cpu'\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRUx_HipsH_T"
      },
      "source": [
        "#@title Set directory\r\n",
        "# Put model file name in ckpt_path's end\r\n",
        "#ex) ckpt_path = parent_dir / 'model.pt'\r\n",
        "result_dir = Path(root) / 'data'\r\n",
        "result_dir.mkdir(parents=True, exist_ok=True)\r\n",
        "\r\n",
        "data_dir = result_dir / 'test_data'\r\n",
        "data_dir.mkdir(parents=True, exist_ok=True)\r\n",
        "\r\n",
        "num_trial=0\r\n",
        "result_dir= Path(root) / 'results'\r\n",
        "parent_dir = result_dir / f'trial_{num_trial}'\r\n",
        "while parent_dir.is_dir():\r\n",
        "    num_trial = int(parent_dir.name.replace('trial_',''))\r\n",
        "    parent_dir = result_dir / f'trial_{num_trial+1}'\r\n",
        " \r\n",
        "# Modify parent_dir here if you want to resume from a checkpoint, or to rename directory.\r\n",
        "# parent_dir = result_dir / 'trial_99'\r\n",
        "\r\n",
        "log_dir = parent_dir\r\n",
        "ckpt_dir = parent_dir\r\n",
        "#################################\r\n",
        "ckpt_path = parent_dir / ''\r\n",
        "#################################\r\n",
        "writer = SummaryWriter(log_dir)\r\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "iqUGizKjt-sz"
      },
      "source": [
        "#@title Helper functions\n",
        "\n",
        "normalize = transforms.Normalize(mean = [0.485, 0.456, 0.406],std = [0.229, 0.224, 0.225])\n",
        "unnormalize = transforms.Normalize(mean = [-2.118, -2.036, -1.804], std = [4.367, 4.464, 4.444])\n",
        "\n",
        "def optical_flow_warp(image, image_optical_flow):\n",
        "    b, _ , h, w = image.size()\n",
        "    grid = np.meshgrid(range(w), range(h))\n",
        "    grid = np.stack(grid, axis=-1).astype(np.float64)\n",
        "    grid[:, :, 0] = grid[:, :, 0] * 2 / (w - 1) -1\n",
        "    grid[:, :, 1] = grid[:, :, 1] * 2 / (h - 1) -1\n",
        "    grid = grid.transpose(2, 0, 1)\n",
        "    grid = np.tile(grid, (b, 1, 1, 1))\n",
        "    grid = Variable(torch.Tensor(grid))\n",
        "    if image_optical_flow.is_cuda == True:\n",
        "        grid = grid.cuda()\n",
        "\n",
        "    flow_0 = torch.unsqueeze(image_optical_flow[:, 0, :, :] * 31 / (w - 1), dim=1)\n",
        "    flow_1 = torch.unsqueeze(image_optical_flow[:, 1, :, :] * 31 / (h - 1), dim=1)\n",
        "    grid = grid + torch.cat((flow_0, flow_1),1)\n",
        "    grid = grid.transpose(1, 2)\n",
        "    grid = grid.transpose(3, 2)\n",
        "    output = F.grid_sample(image, grid, padding_mode='border')\n",
        "    return output\n",
        "\n",
        "def space_to_depth(x, block_size):\n",
        "    n, c, h, w = x.size()\n",
        "    unfolded_x = torch.nn.functional.unfold(x, block_size, stride=block_size)\n",
        "    return unfolded_x.view(n, c * block_size ** 2, h // block_size, w // block_size)\n",
        "\n",
        "def crop(tensor,top,left,h,w): #works!\n",
        "    return tensor[:,:,top:top+h,left:left+w]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siYvQgXQok3Q"
      },
      "source": [
        "# Tensorboard Settings\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbBl6brJopBK"
      },
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "writer = SummaryWriter()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1iZWv-Fk-Bh"
      },
      "source": [
        "# 2. Define Networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kOaLexbitsA",
        "cellView": "code"
      },
      "source": [
        "#@title FlowNet\n",
        "\n",
        "# https://github.com/amanchadha/FRVSR-GAN/blob/master/FRVSRGAN_Models.py\n",
        "class ConvLeaky(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim):\n",
        "        super(ConvLeaky, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=in_dim, out_channels=out_dim,\n",
        "                               kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=out_dim, out_channels=out_dim,\n",
        "                               kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        out = self.conv1(input)\n",
        "        out = F.leaky_relu(out, 0.2)\n",
        "        out = self.conv2(out)\n",
        "        out = F.leaky_relu(out, 0.2)\n",
        "        return out\n",
        "\n",
        "\n",
        "class FNetBlock(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, typ):\n",
        "        super(FNetBlock, self).__init__()\n",
        "        self.convleaky = ConvLeaky(in_dim, out_dim)\n",
        "        if typ == \"maxpool\":\n",
        "            self.final = lambda x: F.max_pool2d(x, kernel_size=2, ceil_mode=True)\n",
        "        elif typ == \"bilinear\":\n",
        "            self.final = lambda x: F.interpolate(x, scale_factor=2, mode=\"bilinear\")\n",
        "        else:\n",
        "            raise Exception('Type does not match any of maxpool or bilinear')\n",
        "\n",
        "    def forward(self, input):\n",
        "        out = self.convleaky(input)\n",
        "        out = self.final(out)\n",
        "        return out\n",
        "\n",
        "# https://arxiv.org/pdf/1801.04590.pdf\n",
        "class FNet(nn.Module):\n",
        "    def __init__(self, in_dim=9):\n",
        "        super(FNet, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            FNetBlock(in_dim, 32, typ=\"maxpool\",),\n",
        "            FNetBlock(32, 64, typ=\"maxpool\"),\n",
        "            FNetBlock(64, 128, typ=\"maxpool\")\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            FNetBlock(128, 256, typ=\"bilinear\"),\n",
        "            FNetBlock(256, 128, typ=\"bilinear\"),\n",
        "            FNetBlock(128, 64, typ=\"bilinear\")\n",
        "        )\n",
        "        self.conv1 = nn.Conv2d(in_channels=64, out_channels=32,\n",
        "                               kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=2,\n",
        "                               kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        _, _ , tmp_x, tmp_y = input.size()\n",
        "        output = self.encoder(input)\n",
        "        output = self.decoder(output) \n",
        "        output = crop(output,0,0, tmp_x, tmp_y)\n",
        "        output = self.conv1(output)\n",
        "        output = F.leaky_relu(output, 0.2)\n",
        "        output = self.conv2(output)\n",
        "        output = torch.tanh(output)\n",
        "        \n",
        "        return output\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gr8bHKjelty4",
        "cellView": "code"
      },
      "source": [
        "#@title Discriminator Net \n",
        "class Discriminator(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Discriminator,self).__init__()\n",
        "    self.l_relu = nn.LeakyReLU()\n",
        "    self.outer_conv = nn.Conv2d(3,64,3)\n",
        "\n",
        "    self.conv_64c = nn.Conv2d(64,64,3,2)\n",
        "    self.bn_64c = nn.BatchNorm2d(64)\n",
        "\n",
        "    self.conv_128e = nn.Conv2d(64,128,3,1)\n",
        "    self.bn_128e = nn.BatchNorm2d(128)\n",
        "\n",
        "    self.conv_128c = nn.Conv2d(128,128,3,2)\n",
        "    self.bn_128c = nn.BatchNorm2d(128)\n",
        "\n",
        "    self.conv_256e = nn.Conv2d(128,256,3,1)\n",
        "    self.bn_256e = nn.BatchNorm2d(256)\n",
        "\n",
        "    self.conv_256c = nn.Conv2d(256,256,3,2)\n",
        "    self.bn_256c = nn.BatchNorm2d(256)\n",
        "\n",
        "    self.conv_512e = nn.Conv2d(256,512,3,1)\n",
        "    self.bn_512e = nn.BatchNorm2d(512)\n",
        "\n",
        "    self.conv_512c = nn.Conv2d(512,512,3,2)\n",
        "    self.bn_512c = nn.BatchNorm2d(512)\n",
        "\n",
        "    self.flatten = nn.AdaptiveAvgPool2d(1)\n",
        "    self.lin1 = nn.Conv2d(512, 1024, kernel_size=1)\n",
        "    self.lin2 = nn.Conv2d(1024, 1, kernel_size=1)\n",
        "    self.sig = nn.Sigmoid()\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.l_relu(self.outer_conv(x))\n",
        "    x = self.l_relu(self.bn_64c(self.conv_64c(x)))\n",
        "    x = self.l_relu(self.bn_128e(self.conv_128e(x)))\n",
        "    x = self.l_relu(self.bn_128c(self.conv_128c(x)))\n",
        "    x = self.l_relu(self.bn_256e(self.conv_256e(x)))\n",
        "    x = self.l_relu(self.bn_256c(self.conv_256c(x)))\n",
        "    x = self.l_relu(self.bn_512e(self.conv_512e(x)))\n",
        "    x = self.l_relu(self.bn_512c(self.conv_512c(x)))\n",
        "\n",
        "    x = self.flatten(x)\n",
        "    x = self.l_relu(self.lin1(x))\n",
        "    x = self.sig(self.lin2(x))\n",
        "\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "sinHlpirmt4-"
      },
      "source": [
        "#@title SRFlowNet\n",
        "\n",
        "class Residual_Block(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(Residual_Block, self).__init__()\n",
        "      self.conv1 = nn.Conv2d(64,64,3,padding=1)\n",
        "      self.conv2 = nn.Conv2d(64,64,3,padding=1)\n",
        "      self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self,x):\n",
        "      rc = x\n",
        "      x = self.conv1(x)\n",
        "      x = self.relu(x)\n",
        "      x = self.conv2(x)\n",
        "     \n",
        "      x = x + rc\n",
        "      return x\n",
        "\n",
        "class SRNet(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(SRNet, self).__init__()\n",
        "      self.conv_init = nn.Conv2d(3,64,3,padding = 1)\n",
        "      self.relu = nn.ReLU()\n",
        "\n",
        "      self.res_thru = nn.Sequential(\n",
        "                Residual_Block(),\n",
        "                Residual_Block(),\n",
        "                Residual_Block(),\n",
        "                Residual_Block(),\n",
        "                Residual_Block(),\n",
        "                Residual_Block(),\n",
        "                Residual_Block(),\n",
        "                Residual_Block(),\n",
        "                Residual_Block(),\n",
        "                Residual_Block()\n",
        "              )\n",
        "      self.ct1 = nn.ConvTranspose2d(64,64,3,2)\n",
        "\n",
        "      self.conv_img = nn.Conv2d(64,3,3,padding = 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = x[1]\n",
        "      shape = x.shape\n",
        "\n",
        "      x = self.conv_init(x) \n",
        "      x = self.relu(x)\n",
        "\n",
        "      residual = x\n",
        "\n",
        "      x = self.res_thru(x)\n",
        "      \n",
        "      x += residual\n",
        "\n",
        "      x = self.ct1(x)\n",
        "      x = self.relu(x)\n",
        "\n",
        "      x = self.conv_img(x)\n",
        "\n",
        "      return crop(x,1,1,shape[2] * 2,shape[3] * 2)\n",
        "\n",
        "class SRFlowNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(SRFlowNet, self).__init__()\n",
        "    self.sr_net = SRNet()\n",
        "    self.conv_init = nn.Conv2d(6,64,3,padding = 1)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.fnet = FNet()\n",
        "\n",
        "    self.res_thru = nn.Sequential(\n",
        "              Residual_Block(),\n",
        "              Residual_Block(),\n",
        "              Residual_Block(),\n",
        "              Residual_Block(),\n",
        "              Residual_Block(),\n",
        "              Residual_Block(),\n",
        "              Residual_Block(),\n",
        "              Residual_Block(),\n",
        "              Residual_Block(),\n",
        "              Residual_Block()\n",
        "            )\n",
        "    self.ct1 = nn.ConvTranspose2d(64,64,3,2)\n",
        "    self.conv_img = nn.Conv2d(64,3,3,padding = 1)\n",
        "\n",
        "  def forward(self, input):\n",
        "    _, x, _ = input\n",
        "    f_x = torch.cat(input, dim=1)\n",
        "\n",
        "    curr = x\n",
        "    fs = self.fnet(f_x)\n",
        "    shape = x.shape\n",
        "    x_warp = optical_flow_warp(curr, fs)\n",
        "    x = torch.cat((x, x_warp), dim=1)\n",
        "\n",
        "    x = self.conv_init(x) \n",
        "    x = self.relu(x)\n",
        "    residual = x\n",
        "    x = self.res_thru(x)\n",
        "    x += residual\n",
        "    x = self.ct1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.conv_img(x)\n",
        "\n",
        "    return crop(x,1,1,shape[2] * 2,shape[3] * 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sl0ZWnn0cTCP"
      },
      "source": [
        "# https://github.com/dongheehand/SRGAN-PyTorch/blob/master/losses.py\n",
        "class perceptual_loss(nn.Module):\n",
        "    def __init__(self, vgg):\n",
        "        super(perceptual_loss, self).__init__()\n",
        "        self.vgg = vgg\n",
        "        self.criterion = nn.MSELoss()\n",
        "    def forward(self, HR, SR, layer = 'relu5_4'):       \n",
        "        hr_feat = self.vgg(HR)\n",
        "        sr_feat = self.vgg(SR)\n",
        "        \n",
        "        return self.criterion(hr_feat, sr_feat), hr_feat, sr_feat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeToItxbfQpF"
      },
      "source": [
        "Define directories for processed data...?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5L-JNv1gMtD"
      },
      "source": [
        "NVIDIA DALI dataloader pipeline configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-e3DuBzgM3C",
        "cellView": "code"
      },
      "source": [
        "#@title DALI video pipline config\n",
        "batch_size=3\n",
        "sequence_length=3\n",
        "\n",
        "initial_prefetch_size=16\n",
        "\n",
        "video_directory = os.path.join(data_dir)\n",
        "video_files = [video_directory + '/' + f for f in os.listdir(video_directory)]\n",
        "\n",
        "shuffle=True\n",
        "args.batch_size = batch_size\n",
        "\n",
        "class VideoPipe(Pipeline):\n",
        "    def __init__(self, batch_size, num_threads, device_id, data, shuffle):\n",
        "        super(VideoPipe, self).__init__(batch_size, num_threads, device_id, seed=16)\n",
        "        self.input = ops.VideoReader(device=\"gpu\", filenames=data, sequence_length=sequence_length,\n",
        "                                     shard_id=0, num_shards=1,\n",
        "                                     random_shuffle=shuffle, initial_fill=initial_prefetch_size,pad_last_batch = True)\n",
        "\n",
        "    def define_graph(self):\n",
        "        output = self.input(name=\"Reader\")\n",
        "        return output\n",
        "\n",
        "\n",
        "pipe = VideoPipe(batch_size=batch_size, num_threads=2, device_id=0, data=video_files, shuffle=shuffle)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtJIDIE3aMaY"
      },
      "source": [
        "# Training Phase"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZxTmuX2b8kq"
      },
      "source": [
        "GAN training Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFyt7wUcSHtn",
        "cellView": "code"
      },
      "source": [
        "#@title GAN training code\n",
        " \n",
        " \n",
        "def train_net_GAN(net,discriminator, resume,n_iter,n_iter_desc,gen_epoc,dis_epoch):\n",
        "    pipe.build()\n",
        "    criterion = nn.MSELoss().to(device) # L2\n",
        "\n",
        "    randcrop = torch.nn.Sequential(\n",
        "        torchvision.transforms.RandomCrop(400, padding=None, pad_if_needed=False, fill=0, padding_mode='constant'),\n",
        "        )\n",
        "    if resume:\n",
        "        checkpoint = torch.load(ckpt_path)\n",
        "        net.load_state_dict(checkpoint['state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        epoch = checkpoint['epoch']\n",
        "        best_valid_iou = checkpoint['best_valid_iou']\n",
        "        print(f'Resume training from epoch {epoch+1}')\n",
        "    else:\n",
        "        epoch = 0\n",
        "\n",
        "    optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, net.parameters()),lr=args.lr)\n",
        "    net.train()\n",
        " \n",
        "    while epoch < gen_epoch: \n",
        "        t1 = time.time()\n",
        "        first = True\n",
        "        loss_total = 0\n",
        "\n",
        "        for i in range(n_iter):\n",
        "            pipe_out = pipe.run() \n",
        "            pipe_out = pipe_out[0].as_cpu().as_array()\n",
        "            pipe_out = randcrop(torch.from_numpy(pipe_out).permute(0,1,4,2,3))\n",
        "\n",
        "            frame_list = torch.chunk(pipe_out,chunks = pipe_out.shape[1],dim=1) \n",
        "\n",
        "            prev_frame =  frame_list[0].squeeze(1).to(device).float()/255.0\n",
        "            frame =       frame_list[1].squeeze(1).float()/255.0       \n",
        "            next_frame =  frame_list[2].squeeze(1).to(device).float()/255.0\n",
        " \n",
        "            h = frame.shape[2]\n",
        "            w = frame.shape[3]\n",
        " \n",
        "            prev_lq =   torch.nn.functional.interpolate(prev_frame,size=(h//2,w//2))\n",
        "            lq_frame =  torch.nn.functional.interpolate(frame,size=(h//2,w//2))\n",
        "            next_lq =   torch.nn.functional.interpolate(next_frame,size=(h//2,w//2))\n",
        " \n",
        "            prev_frame =  normalize(prev_frame).to(device)\n",
        "            frame     =   normalize(frame).to(device)\n",
        "            next_frame =  normalize(next_frame).to(device)\n",
        " \n",
        "            prev_lq =  normalize(prev_lq).to(device)\n",
        "            lq_frame  =   normalize(lq_frame).to(device)\n",
        "            next_lq =  normalize(next_lq).to(device)\n",
        "\n",
        "            frame_recon = net((prev_lq,lq_frame,next_lq))\n",
        "            loss = criterion(frame,frame_recon)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            writer.add_scalar(\"gen-only-loss/train\", loss, epoch*n_iter + i)\n",
        " \n",
        "            loss_total += loss.item()/n_iter\n",
        "       \n",
        "        t = time.time() - t1\n",
        "        \n",
        "        checkpoint = {\n",
        "            'epoch': epoch + 1,\n",
        "            'state_dict': net.state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "        }\n",
        "        torch.save(checkpoint, ckpt_path)\n",
        "        epoch += 1\n",
        "        writer.flush()\n",
        "\n",
        "        print(\"--epoch finished-- \" + str(epoch-1) + \" time taken: \" + str(t))\n",
        "        print(f'loss : {loss_total:.5f}')\n",
        "\n",
        "    vgg_net = torchvision.models.vgg19(pretrained=True).to(device)\n",
        "    vgg_net.eval()\n",
        "    VGG_loss = perceptual_loss(vgg_net)\n",
        "    cross_ent = nn.BCELoss()\n",
        " \n",
        "    real_label = torch.ones((batch_size, 1,1,1)).to(device)\n",
        "    fake_label = torch.zeros((batch_size, 1,1,1)).to(device)\n",
        " \n",
        "    ce_loss = nn.BCELoss()\n",
        " \n",
        "    discriminator.train()\n",
        "    disc_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, discriminator.parameters()),lr=args.lr)\n",
        "\n",
        "    epoch = 0\n",
        "    torch.save(net.state_dict(),  result_dir / f'Attempt_1.ckpt')\n",
        "\n",
        "    print(\"TRAINING DISCRIMINATOR NETWORK\")\n",
        "    while epoch < dis_epoch: \n",
        "        t1 = time.time()\n",
        "        first = True\n",
        "        loss_total = 0\n",
        "        L2_Loss = 0\n",
        "        adv_Loss = 0\n",
        "        disc_loss_total = 0\n",
        "\n",
        "        for i in range(n_iter_desc):\n",
        "            pipe_out = pipe.run()\n",
        "            pipe_out = pipe_out[0].as_cpu().as_array()\n",
        "            pipe_out = randcrop(torch.from_numpy(pipe_out).permute(0,1,4,2,3))\n",
        " \n",
        "            frame_list = torch.chunk(pipe_out,chunks = pipe_out.shape[1],dim=1)\n",
        " \n",
        "            prev_frame =  frame_list[0].squeeze(1).to(device).float()/255.0\n",
        "            frame =       frame_list[1].squeeze(1).float()/255.0    \n",
        "            next_frame =  frame_list[2].squeeze(1).to(device).float()/255.0\n",
        " \n",
        "            h = frame.shape[2]\n",
        "            w = frame.shape[3]\n",
        " \n",
        "            prev_lq =   torch.nn.functional.interpolate(prev_frame,size=(h//2,w//2))\n",
        "            lq_frame =  torch.nn.functional.interpolate(frame,size=(h//2,w//2))\n",
        "            next_lq =   torch.nn.functional.interpolate(next_frame,size=(h//2,w//2))\n",
        " \n",
        "            prev_frame =  normalize(prev_frame).to(device)\n",
        "            frame     =   normalize(frame).to(device)\n",
        "            next_frame =  normalize(next_frame).to(device)\n",
        " \n",
        "            prev_lq =  normalize(prev_lq).to(device)\n",
        "            lq_frame  =   normalize(lq_frame).to(device)\n",
        "            next_lq =  normalize(next_lq).to(device)\n",
        "\n",
        "            frame_recon = net((prev_lq,lq_frame,next_lq))\n",
        "\n",
        "            fake_prob = discriminator(frame_recon)\n",
        "            real_prob = discriminator(frame)\n",
        "            \n",
        "            fake_error = ce_loss(fake_prob,fake_label) \n",
        "            real_error = ce_loss(real_prob,real_label) \n",
        " \n",
        "            disc_loss = fake_error + real_error\n",
        "            disc_optimizer.zero_grad()\n",
        "\n",
        "            percep_loss, hr_feat, sr_feat = VGG_loss((frame + 1.0) / 2.0, (frame_recon + 1.0) / 2.0, layer = 'relu5_4') #re/scaling values to -1 ~ 1, \n",
        "            L2loss = criterion(frame,frame_recon)\n",
        " \n",
        "            percep_loss *= 0.006 \n",
        "            adversarial_loss = cross_ent(fake_prob, real_label) * 1e-3\n",
        " \n",
        "            total_loss = percep_loss + adversarial_loss + L2loss\n",
        "            optimizer.zero_grad()\n",
        " \n",
        " \n",
        "            disc_loss.backward(retain_graph = True)\n",
        " \n",
        "            total_loss.backward()\n",
        "              \n",
        "            disc_optimizer.step()\n",
        "            optimizer.step()\n",
        "\n",
        "            writer.add_scalar(\"adv-total_loss/train\",       total_loss, epoch*n_iter_desc + i)\n",
        "            writer.add_scalar(\"adv-adversarial_loss/train\",  adversarial_loss, epoch*n_iter_desc + i)\n",
        "            writer.add_scalar(\"adv-L2loss/train\",  L2loss, epoch*n_iter_desc + i)\n",
        "            writer.add_scalar(\"adv-disc_loss/train\",  disc_loss, epoch*n_iter_desc + i)\n",
        " \n",
        "            loss_total += total_loss.item()/n_iter\n",
        "            adv_Loss += adversarial_loss.item()/n_iter\n",
        "            L2_Loss += L2loss.item()/n_iter\n",
        "            disc_loss_total += disc_loss.item()/n_iter\n",
        "\n",
        "        t = time.time() - t1\n",
        "\n",
        "        checkpoint = {\n",
        "            'epoch': epoch + 1,\n",
        "            'state_dict': net.state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "        }\n",
        "        torch.save(checkpoint, ckpt_path)\n",
        " \n",
        "        epoch += 1\n",
        "\n",
        "        writer.flush()\n",
        "\n",
        "        print(\"--fine grain epoch finished-- \" + str(epoch-1) + \" time taken: \" + str(t))\n",
        "        print(f'loss : {loss_total:.5f} ,adv_loss :{adv_Loss:.5f},l2_loss: {L2_Loss:.5f} disc_loss :{disc_loss_total:.5f}')\n",
        "\n",
        "    torch.save(net.state_dict(),  result_dir / f'Attempt_1.ckpt')\n",
        "    torch.save(discriminator.state_dict(),  result_dir / f'disc_Attempt_1.ckpt')\n",
        "    writer.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DU6HmIy4Q_Cy",
        "cellView": "code"
      },
      "source": [
        "#@title Set params and Train Model\n",
        " \n",
        "#Set cont as True if you want to continue the training of gernerator based on existing ckpt\n",
        "#load_disc is same as cont but for discriminator\n",
        "#Put generator's file name and discriminator's file name in disc_file and gen_file.\n",
        "#ex) disc_file = 'disc_Attempt_1.ckpt'\n",
        "#ex) gen_file = 'Attempt_1.ckpt'\n",
        "\n",
        "#Set number of epochs and iterations per epoch. ex) gen_epoch=5, gen_iter=100, dis_epoch = 12, dis_iter = 100\n",
        "\n",
        "#Set cont as True if you want to continue the training of gernerator based on existing ckpt\n",
        "#load_disc is same as cont but for discriminator\n",
        "\n",
        "####################################\n",
        "cont = False\n",
        "load_disc = False\n",
        "disc_file = ''\n",
        "gen_file = ''\n",
        " \n",
        "#Set number of epochs and iterations per epoch\n",
        "gen_epoch = \n",
        "gen_iter = \n",
        " \n",
        "dis_epoch = \n",
        "dis_iter =\n",
        "################################3 \n",
        " \n",
        "if cont:\n",
        "\n",
        "  model = SRFlowNet().to(device)\n",
        "  desc = Discriminator().to(device)\n",
        " \n",
        " \n",
        "  model.load_state_dict(torch.load(result_dir / gen_file))\n",
        "  if load_disc:\n",
        "    desc.load_state_dict(torch.load(result_dir / disc_file))\n",
        " \n",
        "  train_net_GAN(model,desc,False,gen_iter,dis_iter,gen_epoch,dis_epoch)\n",
        " \n",
        "else:\n",
        "  model = SRFlowNet().to(device)\n",
        "  desc = Discriminator().to(device)\n",
        " \n",
        "  if load_disc:\n",
        "    desc.load_state_dict(torch.load(result_dir / disc_file))\n",
        " \n",
        "  train_net_GAN(model,desc,False,gen_iter,dis_iter,gen_epoch,dis_epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUso5OBCa0SS"
      },
      "source": [
        "# Model Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhViSRd_fXV3",
        "cellView": "code"
      },
      "source": [
        "#@title Model Test For 3 Image as Input\n",
        "\n",
        "#We checked the place that you should fill with ###########################\n",
        "#imdir is the path for input images.\n",
        "#Since we put 3 images, you should fill imdir1, imdir2, imdir3.\n",
        "\n",
        "#svdir is path for saving result.\n",
        "#ex) svdir = Path(root) / 'valid_test' / 'nalckiep.png'\n",
        "\n",
        "###########################\n",
        "model_file = 'Attempt_1.ckpt'\n",
        "###########################\n",
        "model = SRFlowNet().to(device)\n",
        "model.load_state_dict(torch.load(result_dir / model_file))\n",
        "model.eval()\n",
        " \n",
        "images = []\n",
        "\n",
        "\n",
        "loader = transforms.Compose([transforms.ToTensor()])\n",
        "###########################\n",
        "imdir1 = \n",
        "###########################\n",
        "image1 = cv.imread(str(imdir1))\n",
        "image1 = loader(image1).float()\n",
        "image1 = image1.unsqueeze(0).to(device)\n",
        "images.append(normalize(image1))\n",
        " \n",
        "loader = transforms.Compose([transforms.ToTensor()])\n",
        "###########################\n",
        "imdir2 =\n",
        "###########################\n",
        "image2 = cv.imread(str(imdir2))\n",
        "image2 = loader(image2).float()\n",
        "image2 = image2.unsqueeze(0).to(device)\n",
        "images.append(normalize(image2))\n",
        " \n",
        "loader = transforms.Compose([transforms.ToTensor()])\n",
        "###########################\n",
        "imdir3 =\n",
        "###########################\n",
        "image3 = cv.imread(str(imdir3))\n",
        "image3 = loader(image3).float()\n",
        "image3 = image3.unsqueeze(0).to(device)\n",
        "images.append(normalize(image3))\n",
        " \n",
        "\n",
        "n_image = model(images)\n",
        "n_image = unnormalize(n_image)\n",
        " \n",
        " \n",
        "n_image2 = n_image.squeeze(0).transpose(0,2).transpose(0,1).cpu().detach().numpy()\n",
        "n_image2 =(n_image2*255).astype(np.uint8)\n",
        "###########################\n",
        "svdir =\n",
        "###########################\n",
        "cv.imwrite(str(svdir),n_image2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlV7yTc0sABJ"
      },
      "source": [
        "#@title Model Test For Video as Input\r\n",
        "#If you want to put video data as input, try this code below.\r\n",
        "\r\n",
        "#Put video file as input in data.\r\n",
        "#ex) data='/gdrive/My Drive/Final_Project/data/test_data/Video1.mp4'\r\n",
        "#Put where to save your frames of your video.to \"HERE\"\r\n",
        "#We hardly recommend you to MAKE \"new foler\" for frames.\r\n",
        "#DO NOT change part after the path in name.\r\n",
        "#ex)  name= '/gdrive/My Drive/Final_Project/data/test_data/frame/'+str(Number)+'.jpg'\r\n",
        "###########################\r\n",
        "video_filename=''\r\n",
        "###################################\r\n",
        "\r\n",
        "data = root + 'test/Before/' + video_filename\r\n",
        "after = root + 'test/After/frames'\r\n",
        "frames = root + 'test/Before/frames'\r\n",
        "\r\n",
        "result = root + 'test/After/'\r\n",
        "\r\n",
        "\r\n",
        "Path(after).mkdir(parents=True, exist_ok=True)\r\n",
        "Path(frames).mkdir(parents=True, exist_ok=True)\r\n",
        "\r\n",
        "\r\n",
        "vc=cv.VideoCapture(data)\r\n",
        "Number=0\r\n",
        "\r\n",
        "while(True):\r\n",
        "  ret, frame=vc.read()\r\n",
        "  if not (ret):\r\n",
        "    break\r\n",
        "  #####################################  \r\n",
        "  name= frames + str(Number)+'.jpg'\r\n",
        "  #########################################\r\n",
        "  cv.imwrite(name,frame)\r\n",
        "  Number+=1\r\n",
        "\r\n",
        "vc.release()\r\n",
        "cv.destroyAllWindows()\r\n",
        "\r\n",
        "\r\n",
        "#Set model_file as .ckpt file\r\n",
        "# ex) model_file = Path(root)/'Models'/'disc_IFNet_1129.ckpt'\r\n",
        "\r\n",
        "#model_file ='/gdrive/My Drive/Final_Project/results/disc_IFNet_.ckpt'\r\n",
        "#You should modify the path mentioned as \"HERE\" as same as upward.\r\n",
        "#\"THERE\" is the path where you save the result frame.\r\n",
        "#If you don't mine original picture overwritten, you can just use same path for \"HERE\" and \"THERE\".\r\n",
        "#The path in \"HERE\", \"THERE\" SHOULD BE SAME!!!!\r\n",
        "#ex)change \"HERE\" as '/gdrive/My Drive/Final_Project/data/test_data/frame'\r\n",
        "#DO NOT modify part after \"HERE\" and \"THERE\"\r\n",
        "#Modify (size) as the size you want.\r\n",
        "#(size) SHOULD BE SAME and format of (x,y) and each x and y SHOULD BE DIVIDED TO 8!!!!!\r\n",
        "#ex) (800,800)\r\n",
        "\r\n",
        "##########################\r\n",
        "model_file ='Attempt_1.ckpt'\r\n",
        "##################################\r\n",
        "model = IFNet().to(device)\r\n",
        "model.load_state_dict(torch.load(result_dir / model_file))\r\n",
        "model.eval()\r\n",
        "\r\n",
        "frame_list=os.listdir(frames)\r\n",
        "\r\n",
        "# Images need to be multiple of 16 to operate in our network.\r\n",
        "image = cv.imread(frames + str(0)+'.jpg')\r\n",
        "shape = image.shape\r\n",
        "w_left = shape[0] % 16\r\n",
        "h_left = shape[1] % 16\r\n",
        "w_new = w - w_left\r\n",
        "h_new = h - h_left\r\n",
        "size = (w_new,h_new)\r\n",
        "\r\n",
        "for i in range(1,len(frame_list)-1):\r\n",
        "  images=[]\r\n",
        "  \r\n",
        "  image1= cv.imread(frames + str(i-1)+'.jpg')\r\n",
        "  image1=cv.resize(image1,(size) )\r\n",
        "  image2= cv.imread(frames + str(i)+'.jpg')\r\n",
        "  image2=cv.resize(image2,(size) )\r\n",
        "  image3= cv.imread(frames + str(i+1)+'.jpg')\r\n",
        "  image3=cv.resize(image3,(size) )\r\n",
        "  \r\n",
        "  image1_=image1\r\n",
        "  image2_=image2\r\n",
        "  image3_=image3\r\n",
        "\r\n",
        "  loader = transforms.Compose([transforms.ToTensor()])\r\n",
        "  image1 = loader(image1).float()\r\n",
        "  image1 = image1.unsqueeze(0).to(device)\r\n",
        "  images.append(normalize(image1))\r\n",
        "\r\n",
        "  loader = transforms.Compose([transforms.ToTensor()])\r\n",
        "  image2 = loader(image2).float()\r\n",
        "  image2 = image2.unsqueeze(0).to(device)\r\n",
        "  images.append(normalize(image2))\r\n",
        "\r\n",
        "  loader = transforms.Compose([transforms.ToTensor()])\r\n",
        "  image3 = loader(image3).float()\r\n",
        "  image3 = image3.unsqueeze(0).to(device)\r\n",
        "  images.append(normalize(image3))\r\n",
        "\r\n",
        "  n_image = model(images)\r\n",
        "  n_image = unnormalize(n_image)\r\n",
        " \r\n",
        "  n_image2 = n_image.squeeze(0).transpose(0,2).transpose(0,1).cpu().detach().numpy()\r\n",
        "  n_image2 =(n_image2*255).astype(np.uint8)\r\n",
        "\r\n",
        "  svname= after + str(i)+'.jpg'\r\n",
        "  cv.imwrite(svname,n_image2)\r\n",
        "\r\n",
        "\r\n",
        "#in VideoWriter, you should put code where to save your result video file.\r\n",
        "#ex) video=cv2.VideoWriter('/gdrive/My Drive/VideoCapture/capture/Hi.avi',fourcc,15, (800,600))\r\n",
        "\r\n",
        "#fourccis avi file's codec. Divx, Xvid, H.264 is available if you set your video file as .avi.\r\n",
        "#If you want your output as .mp4 change fourcc 'XVID' as'mp4v' and change your video file name as .mp4.\r\n",
        "#You can change FPS of video as you want.\r\n",
        "\r\n",
        "#########################\r\n",
        "fourcc = cv.VideoWriter_fourcc(*'XVID')\r\n",
        "FPS= 15 # Default\r\n",
        "video=cv.VideoWriter( result + '/Result.avi',fourcc,FPS, size)\r\n",
        "\r\n",
        "n=os.listdir(after)\r\n",
        "###################################\r\n",
        "\r\n",
        "\r\n",
        "n = sorted(n)\r\n",
        "\r\n",
        "for i in range (len(n)):\r\n",
        "  img=cv.imread(after+n[i])\r\n",
        "  video.write(img)\r\n",
        "\r\n",
        "video.release()\r\n",
        "cv.destroyAllWindows()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}